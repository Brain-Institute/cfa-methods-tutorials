{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d6e547e-89dc-4a92-97bd-faaa0e0074d2",
   "metadata": {},
   "source": [
    "# Cove ADHD-ASD analysis: learning latent constructs of brain activity using connectivity maps and variational autoencoders (VAEs)\n",
    "\n",
    "In this notebook, we will be walking through the analysis of neurophysiological and neuroimaging data from individuals with autism spectrum disorders (ASD), attention deficit hyperactivity disorder (ADHD), as well as typically developing (TD) children. The goals of this notebook are twofold:\n",
    "1. Preprocess magnetoencephalography (MEG) and functional magnetic resonance imaging (fMRI) data collected from children with either ASD, ADHD, or typical development histories, using best-practice approaches via commercially-friendly software implementations.\n",
    "2. Extract connectivity networks\n",
    "3. Fit neural network models (variational autoencoders (VAEs)) to the spatiotemporal connectivity networks\n",
    "4. Relate the learned latent distributions to clinical outcomes and identify subgroups of patients within- and/or across diagnostic categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55768a3-3253-48ba-a011-51720fe5f03f",
   "metadata": {},
   "source": [
    "# First, let's compile our clinical data\n",
    "\n",
    "This information will give us some intuition about the distributions of clinical severities among the patients, and also how they relate to each other at a high level (e.g., we can use PCA to explore the overall distributions of Strengths and Weaknesses of ADHD symptoms and Normal behavior (SWAN) evaluation scores). We can step through each of the populations and save a condensed clinical sheet for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646e3156-0d43-4f52-b533-97c0e60ec381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import seaborn as sns\n",
    "from functools import reduce\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "clinical_files = glob.glob(\"/home/coveneuro-leif/Downloads/**/Clinical*\", recursive=True)\n",
    "clinical_files = [c for c in clinical_files if \".csv\" in c]\n",
    "clinical_dfs = [pd.read_csv(c, encoding=\"utf-16\") for c in clinical_files]\n",
    "clinical_main_df = pd.concat(clinical_dfs)\n",
    "\n",
    "clinical_main_df.columns\n",
    "clinical_main_df['subject_id'].unique()\n",
    "clinical_main_df.groupby([\"subject_id\", \"primary_diagnosis\"]).count()\n",
    "\n",
    "### SWAN assessment\n",
    "clinical_SWAN = clinical_main_df.loc[clinical_main_df['form_name'].isin([\"SWAN\"]), :]\n",
    "\n",
    "dfs = []\n",
    "for qnum in np.arange(18):\n",
    "    question = f\"P{qnum+1}\"\n",
    "    clinical_SWAN_Q = clinical_SWAN.loc[clinical_SWAN['field_name'].isin([f\"SWAN{question}\"]), :]\n",
    "    \n",
    "    clinical_SWAN_Q.index = clinical_SWAN_Q['subject_id']\n",
    "    title = clinical_SWAN_Q['field_desc'].values[0]\n",
    "\n",
    "    if qnum==0:\n",
    "        clinical_SWAN_Q = pd.DataFrame(clinical_SWAN_Q[['primary_diagnosis', 'field_value']].values, columns=['primary_diagnosis', title], index=clinical_SWAN_Q.index)\n",
    "    else:\n",
    "        clinical_SWAN_Q = pd.DataFrame(clinical_SWAN_Q[['field_value']].values, columns=[title], index=clinical_SWAN_Q.index)\n",
    "    dfs.append(clinical_SWAN_Q)\n",
    "\n",
    "# Recode SWAN ratings into ordinal values\n",
    "clinical_SWAN_final = pd.concat(dfs, axis=1)\n",
    "clinical_SWAN_final_facto = clinical_SWAN_final.copy()\n",
    "for c in clinical_SWAN_final.columns[1:]:\n",
    "    clinical_SWAN_final[c] = pd.Categorical(clinical_SWAN_final[c].values, \n",
    "                                                      categories = ['Far above', 'Above', 'Slightly above', 'Average', 'Slightly below',\n",
    "                                                                    'Below', 'Far below', 'Blank'])\n",
    "    clinical_SWAN_final_facto.loc[clinical_SWAN_final_facto[c]==\"Far above\", c]         = 0\n",
    "    clinical_SWAN_final_facto.loc[clinical_SWAN_final_facto[c]==\"Above\", c]             = 1\n",
    "    clinical_SWAN_final_facto.loc[clinical_SWAN_final_facto[c]==\"Slightly above\", c]    = 2\n",
    "    clinical_SWAN_final_facto.loc[clinical_SWAN_final_facto[c]==\"Average\", c]           = 3\n",
    "    clinical_SWAN_final_facto.loc[clinical_SWAN_final_facto[c]==\"Slightly below\", c]    = 4\n",
    "    clinical_SWAN_final_facto.loc[clinical_SWAN_final_facto[c]==\"Below\", c]             = 5\n",
    "    clinical_SWAN_final_facto.loc[clinical_SWAN_final_facto[c]==\"Far below\", c]         = 6\n",
    "    clinical_SWAN_final_facto.loc[clinical_SWAN_final_facto[c]==\"Blank\", c]             = 7\n",
    "    \n",
    "clinical_SWAN_final_facto.dropna(axis=0, inplace=True)\n",
    "clinical_SWAN_final_facto[clinical_SWAN_final_facto.columns[1:]] = clinical_SWAN_final_facto.iloc[:, 1:].astype(int)\n",
    "\n",
    "qnum = 17\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.suptitle(f\"(P{qnum+1}) \"+clinical_SWAN_final.columns[qnum+1])\n",
    "plt.subplots_adjust(bottom=0.3)\n",
    "sns.histplot(data=clinical_SWAN_final, x = clinical_SWAN_final.columns[qnum+1], hue = \"primary_diagnosis\", \n",
    "             kde=True, multiple=\"dodge\", stat=\"percent\", common_norm=False, ax = ax)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "ax.set_xlabel('')\n",
    "\n",
    "### Exploratory plotting of SWAN scores and their clusters\n",
    "clf = KMeans(n_clusters=3)\n",
    "clf.fit(clinical_SWAN_final_facto.iloc[:, 1:])\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_out = pca.fit_transform(clinical_SWAN_final_facto.iloc[:, 1:])\n",
    "plt.scatter(pca_out[:, 0], pca_out[:, 1], c = clf.labels_)\n",
    "plt.scatter(pca_out[:, 0], pca_out[:, 1], c = clinical_SWAN_final_facto.iloc[:, 0].factorize()[0])\n",
    "\n",
    "from scipy.stats import mannwhitneyu, ttest_ind\n",
    "from itertools import combinations\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "unique_pairs = list(combinations([\"Typically-Developing\", \"ADHD\", \"ASD\"], 2))\n",
    "\n",
    "pvals = dict()\n",
    "stats = dict()\n",
    "for pop1, pop2 in unique_pairs:\n",
    "    pvals_temp = []\n",
    "    stats_temp = []\n",
    "    for c in clinical_SWAN_final_facto.columns[1:]:\n",
    "        pval = ttest_ind(clinical_SWAN_final_facto.loc[clinical_SWAN_final_facto['primary_diagnosis']==pop1, c].values,\n",
    "                            clinical_SWAN_final_facto.loc[clinical_SWAN_final_facto['primary_diagnosis']==pop2, c].values)[1]\n",
    "        pvals_temp.append(pval)\n",
    "        stat = ttest_ind(clinical_SWAN_final_facto.loc[clinical_SWAN_final_facto['primary_diagnosis']==pop1, c].values,\n",
    "                            clinical_SWAN_final_facto.loc[clinical_SWAN_final_facto['primary_diagnosis']==pop2, c].values)[0]\n",
    "        stats_temp.append(stat)\n",
    "    pvals.update({pop1+\"_\"+pop2: np.array(pvals_temp)})\n",
    "    stats.update({pop1+\"_\"+pop2: np.array(stats_temp)})\n",
    "\n",
    "stats_df = pd.DataFrame(stats)\n",
    "pvals_df = pd.DataFrame(pvals)\n",
    "pvals_corr = fdrcorrection(pvals_df.values.ravel(), alpha = 0.05)[1].reshape(pvals_df.shape)\n",
    "pvals_corr_df = pd.DataFrame(pvals_corr, index = clinical_SWAN_final_facto.columns[1:], columns = pvals_df.columns)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20,10))\n",
    "plt.subplots_adjust(left=0.3, bottom=0.3)\n",
    "ax.set_title(\"log10(p-values) and test statistics of pairwise t-tests - SWAN items\\nLower=first group is higher-functioning\\nCells with values significant after FDR\")\n",
    "sns.heatmap(np.log10(pvals_corr_df), mask = pvals_corr_df>0.05, annot=stats_df.values)\n",
    "\n",
    "### Demographics\n",
    "data_table = []\n",
    "clin_pops = [\"Typically-Developing\", \"ADHD\"]\n",
    "\n",
    "pop = \"ADHD\" # \"Typically-Developing\", \"ASD\", \"ADHD\"\n",
    "clinical_file = f\"/home/coveneuro-leif/Downloads/Clinical_Demographics_{pop}/Clinical_Demographics_{pop}.csv\"\n",
    "data_dir = f\"/home/coveneuro-leif/Downloads/Imaging_MEG-RestingState_{pop}/\"\n",
    "\n",
    "# Load data\n",
    "clinical_data = pd.read_csv(clinical_file, encoding=\"utf-16\")\n",
    "# clinical_data.columns\n",
    "\n",
    "# Get data filenames. Also get shortened subject_id's that correspond to the formatting in the clinical datasheet\n",
    "all_files = glob.glob(data_dir+\"/*\")\n",
    "all_files_subs = [\"_\".join(a.split(\"/\")[-1].split(\"_\")[:3]) for a in all_files]\n",
    "\n",
    "# For grabbing the correct rows from the main df\n",
    "selector = clinical_data['subject_id'].isin(all_files_subs)\n",
    "clinical_data_meg = clinical_data.loc[selector, :]\n",
    "\n",
    "# Demographics\n",
    "# Sex\n",
    "demos_sex = clinical_data_meg.loc[clinical_data_meg['field_name'].isin(['GENDER']),:]\n",
    "prc_female = (demos_sex['field_value']==\"Female\").mean()\n",
    "\n",
    "# Age\n",
    "demos_age = clinical_data_meg.loc[clinical_data_meg['field_name'].isin(['AGE_AT_ENROLLMENT']),:]\n",
    "age_median = demos_age['field_value'].astype(int).median()\n",
    "age_iqr_lo = demos_age['field_value'].astype(int).quantile(0.25)\n",
    "age_iqr_hi = demos_age['field_value'].astype(int).quantile(0.75)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.hist(demos_age['field_value'].astype(int))\n",
    "# plt.title(f\"Age-{pop}\")\n",
    "# plt.savefig(f\"hist-{pop}.png\")\n",
    "\n",
    "data_list = [prc_female, age_median, age_iqr_lo, age_iqr_hi]\n",
    "data_table.append(data_list)\n",
    "\n",
    "data_table_df = pd.DataFrame(data_table).T\n",
    "data_table_df.index = [\"% Female\", \"Age (median)\", \"Age (IQR-lower)\", \"Age (IQR-upper)\"]\n",
    "data_table_df.columns = clin_pops\n",
    "\n",
    "# %% Clinical scores\n",
    "\n",
    "clinical_file = f\"/home/coveneuro-leif/Downloads/Clinical_Demographics_{pop}/Clinical_Demographics_{pop}.csv\"\n",
    "data_dir = f\"/home/coveneuro-leif/Downloads/Imaging_MEG-RestingState_{pop}/\"\n",
    "\n",
    "# Load data\n",
    "clinical_data = pd.read_csv(clinical_file, encoding=\"utf-16\")\n",
    "# clinical_data.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668377f4-6db0-42b7-927e-8244f6453481",
   "metadata": {},
   "source": [
    "# Next, let's get into preprocessing and network extraction\n",
    "\n",
    "Here, we will load the files that we want to analyze (MEG first, then fMRI). Load all libraries and grab MEG files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c623fe-c496-41e5-bcf4-5fac2409535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lots of the below from the MNE tutorial here: https://mne.tools/stable/auto_tutorials/preprocessing/40_artifact_correction_ica.html\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import mne\n",
    "import mne_connectivity\n",
    "from mne.preprocessing import ICA, corrmap\n",
    "from mne.preprocessing import (\n",
    "    compute_proj_ecg,\n",
    "    compute_proj_eog,\n",
    "    create_ecg_epochs,\n",
    "    create_eog_epochs,\n",
    ")\n",
    "import autoreject\n",
    "import matplotlib.pyplot as plt\n",
    "from mne import write_source_spaces\n",
    "\n",
    "# Load data - assumes shuttle_MEG_files.py has been run on the main_dir target\n",
    "main_dir    = \"/home/coveneuro-leif/Downloads/Imaging_MEG-RestingState_Typically-Developing/\"\n",
    "file_idx    = 0\n",
    "all_files_TD   = glob.glob(main_dir + \"/*\")\n",
    "all_files_TD   = [a for a in all_files_TD if \".ds\" in a]\n",
    "\n",
    "# Load data - assumes shuttle_MEG_files.py has been run on the main_dir target\n",
    "main_dir    = \"/home/coveneuro-leif/Downloads/Imaging_MEG-RestingState_ASD/\"\n",
    "file_idx    = 0\n",
    "all_files_ASD   = glob.glob(main_dir + \"/*\")\n",
    "all_files_ASD   = [a for a in all_files_ASD if \".ds\" in a]\n",
    "\n",
    "# Load data - assumes shuttle_MEG_files.py has been run on the main_dir target\n",
    "main_dir    = \"/home/coveneuro-leif/Downloads/Imaging_MEG-RestingState_ADHD/\"\n",
    "file_idx    = 0\n",
    "all_files_ADHD   = glob.glob(main_dir + \"/*\")\n",
    "all_files_ADHD   = [a for a in all_files_ADHD if \".ds\" in a]\n",
    "\n",
    "all_files = all_files_TD+all_files_ASD+all_files_ADHD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e374b3a7-190f-472c-ad65-42510667a144",
   "metadata": {},
   "source": [
    "Now let's iterate through and perform a first processing pass of all MEG files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570aaa4e-e91c-4d78-847a-750a467b7176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Overall workflow\n",
    "\n",
    "# Load data\n",
    "# Get overlapping channel names across the max number of samples\n",
    "# Recognizing that many are not shared (but only off by 1 or 2 usually,\n",
    "# except for one weirdo sample that uses ENTIRELY different channel names)\n",
    "chs  = list()\n",
    "for idx_f, f in enumerate(all_files):\n",
    "    # break\n",
    "    raw         = mne.io.read_raw_ctf(f, preload=True).pick_types(\"mag\")\n",
    "    chs.append(pd.DataFrame(data=[1]*len(raw.info['ch_names']), index=raw.info['ch_names']))\n",
    "chs_ = pd.concat(chs, axis=1)\n",
    "chs_keep = list(chs_.index[chs_.sum(axis=1)>=58].values)\n",
    "chs_keep = [c for c in chs_keep if \"MRP21\" not in c]\n",
    "# chs_keep_4602 = [c.replace(\"1706\", \"4602\").replace(\"2104\", \"4122\").replace(\"3000\", \"4122\") for c in chs_keep]\n",
    "\n",
    "# raw.info.rename_channels(mapping={c: c.split(\"-\")[0] for c in raw.info['ch_names']})\n",
    "chs_keep_2 = [c.split(\"-\")[0] for c in chs_keep]\n",
    "n_comps = 15\n",
    "\n",
    "# Perform batch processing up to the point of ICA\n",
    "for idx_f, f in enumerate(all_files):\n",
    "    '''\n",
    "    idx_f = 13 \n",
    "    f=all_files[idx_f]\n",
    "    '''\n",
    "    # try:\n",
    "    # Read in, select MAG channels only, resample (less memory etc., doesn't need to be at 600)\n",
    "    # raw         = mne.io.read_raw_ctf(f, preload=True).resample(250).pick_channels(chs_keep).pick_types(\"mag\")\n",
    "    # raw         = mne.io.read_raw_ctf(f, preload=True).resample(250).pick_channels(chs_keep_2).pick_types(\"mag\")\n",
    "    raw         = mne.io.read_raw_ctf(f, preload=True).resample(250)\n",
    "    raw.info.rename_channels(mapping={c: c.split(\"-\")[0] for c in raw.info['ch_names']})\n",
    "    raw         = raw.pick_types(\"mag\").pick_channels(chs_keep_2)\n",
    "    # raw.info['ch_names']\n",
    "    # Filter data - notch and then BP\n",
    "    # The 120 is arguably redundant here - remove in future to save time?\n",
    "    filtered    = raw.copy().notch_filter([60, 120])\n",
    "    filtered    = filtered.copy().filter(l_freq=1, h_freq = 100)\n",
    "    filtered.save(f\"{f.replace('_meg.ds', '_filtered_meg.fif')}\", overwrite=True)\n",
    "    \n",
    "    # ICA\n",
    "    ica         = ICA(n_components=n_comps, max_iter=100, random_state=97)\n",
    "    ica.fit(filtered, verbose=\"error\")\n",
    "    ica.save(f\"{f.replace('_meg.ds', '_ica.fif')}\", overwrite=True)\n",
    "    ica_txed    = ica._transform_raw(filtered, 0, len(filtered.times)) # Sneaky\n",
    "    \n",
    "    new_dir     = f.replace(\".ds\", \"_scan\")#\"/\".join(f.split(\"/\")[:-1]) + f\"/scan_{idx_f}\"\n",
    "    \n",
    "    # Make a new directory to save processed data if it doesn't already exist\n",
    "    if os.path.isdir(new_dir)==False:\n",
    "        os.mkdir(new_dir)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    for i in range(n_comps):\n",
    "        fig, ax = plt.subplots(1,1, figsize=(1.2, 1.2))\n",
    "        ica.plot_components(picks=i, axes=ax, show=False, title=\"\",\n",
    "                            outlines=None, sensors=False, contours=0)\n",
    "        ax.set_title(\"\")\n",
    "         # TODO MEGnet cuts timeseries into epochs but uses the same spatial map repeatedly\n",
    "         # NOT recalculated per-epoch\n",
    "        timeseries = ica_txed[i, :]\n",
    "        \n",
    "        win_len     = 15000\n",
    "        overlap     = 3750\n",
    "        start_times = []\n",
    "        st          = 0\n",
    "        k           = 0\n",
    "        while st+win_len<len(timeseries):\n",
    "            # We append first and increment next to ensure there is (win_len) \n",
    "            # of room remaining to index into later!\n",
    "            start_times.append(st)\n",
    "            # print(st, st+win_len)\n",
    "            \n",
    "            plt.savefig(new_dir+f\"/map_{i}_epoch_{k}.png\")\n",
    "            np.save(new_dir+f\"/timeseries_{i}_epoch_{k}\", timeseries[st:(st+win_len)])\n",
    "            \n",
    "            st += win_len-overlap\n",
    "            k += 1\n",
    "            \n",
    "        plt.close()\n",
    "    # except:\n",
    "        # pass\n",
    "    # ica         = mne.preprocessing.read_ica(f\"{f.replace('_meg.ds', '_ica.fif')}\")\n",
    "    # ica.plot_sources(filtered) # Clicking automatically excludes those components\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ece59d-fc2e-4d40-91e6-f72fa68e4e69",
   "metadata": {},
   "source": [
    "Here, we manually iterate through the files and perform ICA. The workflow is split into the cell below this one, and then the one below that one: run the first cell to load the ICA object thta was saved in the previous iterative loop, as well as the filtered data. Then visually inspect for data quality. To reconstruct & save the data, once it is properly cleaned, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50290c1b-1d01-42b3-a315-4fdb3f09ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Perform single processing to (1) get bad ICs, (2) reconstruct low-dim data, (3) save reconstructed data\n",
    "\n",
    "ix          = 58\n",
    "idx_f, f    = ix, all_files[ix]\n",
    "filtered    = mne.io.read_raw_fif(f\"{f.replace('_meg.ds', '_filtered_meg.fif')}\", preload=True)\n",
    "ica         = mne.preprocessing.read_ica(f\"{f.replace('_meg.ds', '_ica.fif')}\")\n",
    "ica.plot_sources(filtered) # Clicking automatically excludes those components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576093e3-c243-4214-a0f3-7f239221f3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconst = filtered.copy()\n",
    "ica.apply(reconst)\n",
    "reconst.save(f\"{f.replace('_meg.ds', '_ica15_reconst_meg.fif')}\", overwrite=True)\n",
    "# reconst.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c21dc6f-5d65-4efb-baf6-1fa9789e9172",
   "metadata": {},
   "source": [
    "Perform autoreject to remove bad epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e65b0-42c1-4d20-8991-3ca6181ac5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Iterate over all reconstructed files and perform autoreject\n",
    "\n",
    "# Perform batch processing up to the point of ICA\n",
    "for idx_f, f in enumerate(all_files):\n",
    "    if os.path.isfile(f\"{f.replace('_meg.ds', '_ica15_reconst_meg_ar_epo.fif')}\")==True:\n",
    "        continue\n",
    "    else:\n",
    "        print(idx_f)\n",
    "        try:\n",
    "            reconst = mne.io.read_raw_fif(f\"{f.replace('_meg.ds', '_ica15_reconst_meg.fif')}\")\n",
    "            \n",
    "            # Make into epochs for autoreject\n",
    "            epochs = mne.make_fixed_length_epochs(reconst, duration=3, preload=True)#, picks=mag_channels)\n",
    "            \n",
    "            # Autoreject\n",
    "            ar = autoreject.AutoReject(n_jobs=-1, verbose=True)\n",
    "            ar.fit(epochs)\n",
    "            epochs_ar, reject_log = ar.transform(epochs, return_log=True)\n",
    "            epochs_ar.save(f\"{f.replace('_meg.ds', '_ica15_reconst_meg_ar_epo.fif')}\", overwrite=True)\n",
    "            # epochs_ar.plot()\n",
    "            # break\n",
    "        except:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74158eb2-7c0f-44e6-b41b-a50431f7ad31",
   "metadata": {},
   "source": [
    "Now we perform source modeling. This is one of the most powerful aspects of MEG, owing to its high-density ata collection ability and relative lack of noise due to volume effects (cf. EEG, where this is a major problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2b0ba5-c5be-4ad9-8089-9c174a253f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for idx_f, f in enumerate(all_files):\n",
    "    # break\n",
    "    if os.path.isfile(f\"{f.replace('_meg.ds', '_ica15_reconst_meg_ar_epo.fif')}\")==False:\n",
    "        continue\n",
    "    else:\n",
    "        \n",
    "        epo         = mne.read_epochs(f\"{f.replace('_meg.ds', '_ica15_reconst_meg_ar_epo.fif')}\") \n",
    "        info        = epo.info#raw.info\n",
    "        \n",
    "        noise_cov   = mne.make_ad_hoc_cov(info)\n",
    "        parc        = \"aparc\"  # the parcellation to use, e.g., 'aparc' 'aparc.a2009s'\n",
    "        loose       = dict(surface=0.2, volume=1.0)\n",
    "        snr         = 3.0  # use smaller SNR for raw data\n",
    "        lambda2     = 1.0 / snr**2\n",
    "        \n",
    "        data_path = mne.datasets.sample.data_path()\n",
    "        subject = \"sample\"\n",
    "        data_dir = data_path / \"MEG\" / subject\n",
    "        subjects_dir = data_path / \"subjects\"\n",
    "        bem_dir = subjects_dir / subject / \"bem\"\n",
    "        \n",
    "        # Set file names\n",
    "        fname_mixed_src = bem_dir / f\"{subject}-oct-6-mixed-src.fif\"\n",
    "        fname_aseg = subjects_dir / subject / \"mri\" / \"aseg.mgz\"\n",
    "        \n",
    "        fname_model = bem_dir / f\"{subject}-5120-bem.fif\"\n",
    "        fname_bem = bem_dir / f\"{subject}-5120-bem-sol.fif\"\n",
    "        \n",
    "        fname_evoked = data_dir / f\"{subject}_audvis-ave.fif\"\n",
    "        fname_trans = data_dir / f\"{subject}_audvis_raw-trans.fif\"\n",
    "        # fname_fwd = data_dir / f\"{subject}_audvis-meg-oct-6-mixed-fwd.fif\"\n",
    "        fname_cov = data_dir / f\"{subject}_audvis-shrunk-cov.fif\"\n",
    "        \n",
    "        \n",
    "        # import os.path as op\n",
    "        # from mne.datasets import fetch_fsaverage\n",
    "        # fs_dir = fetch_fsaverage(verbose=True)\n",
    "        # subjects_dir = op.dirname(fs_dir)\n",
    "        # subject = 'fsaverage'\n",
    "        # trans = 'fsaverage'\n",
    "        # src = op.join(fs_dir, 'bem', 'fsaverage-ico-5-src.fif')\n",
    "        # fname_bem = op.join(fs_dir, 'bem', 'fsaverage-5120-5120-5120-bem-sol.fif')\n",
    "        \n",
    "        labels_vol = [\n",
    "            \"Left-Amygdala\",\n",
    "            \"Left-Thalamus-Proper\",\n",
    "            \"Left-Cerebellum-Cortex\",\n",
    "            \"Brain-Stem\",\n",
    "            \"Right-Amygdala\",\n",
    "            \"Right-Thalamus-Proper\",\n",
    "            \"Right-Cerebellum-Cortex\",\n",
    "        ]\n",
    "        \n",
    "        ''' # Run the commented stuff here IF YOU WANT TO REMAKE THE INVERSE OPERATOR AND SAVE IT\n",
    "        # Get a surface-based source space, here with few source points for speed in this demonstration, in general you should use oct6 spacing!\n",
    "        src = mne.setup_source_space(\n",
    "            subject, spacing=\"oct6\", add_dist=False, subjects_dir=subjects_dir\n",
    "        )\n",
    "        vol_src = mne.setup_volume_source_space(\n",
    "            subject,\n",
    "            mri=fname_aseg,\n",
    "            pos=10.0,\n",
    "            bem=fname_model,\n",
    "            volume_label=labels_vol,\n",
    "            subjects_dir=subjects_dir,\n",
    "            add_interpolator=False,  # just for speed, usually this should be True\n",
    "            verbose=True,\n",
    "        )\n",
    "        \n",
    "        # Generate the mixed source space\n",
    "        src += vol_src\n",
    "        write_source_spaces(fname_mixed_src, src, overwrite=True)\n",
    "        \n",
    "        \n",
    "        fwd         = mne.make_forward_solution(info, trans=fname_trans, \n",
    "                                                src=fname_mixed_src, \n",
    "                                                bem=fname_bem, meg=True, eeg=False, \n",
    "                                                mindist=0.0, ignore_ref=False, \n",
    "                                                n_jobs=None, verbose=None)\n",
    "        inv         = mne.minimum_norm.make_inverse_operator(info, fwd, \n",
    "                                                             noise_cov, depth=None, \n",
    "                                                             loose=loose, verbose=True)\n",
    "        del fwd\n",
    "        mne.minimum_norm.write_inverse_operator(\"/home/coveneuro-leif/Documents/scripts/inv\", inv, overwrite=True)\n",
    "        '''\n",
    "        epo.info['ch_names']\n",
    "        epo.rename_channels({c: c.split(\"-\")[0] for c in epo.info['ch_names']})\n",
    "        # inv.ch_names\n",
    "        inv         = mne.minimum_norm.read_inverse_operator(\"/home/coveneuro-leif/Documents/scripts/inv\")\n",
    "        stc         = mne.minimum_norm.apply_inverse_epochs(epo, \n",
    "                                                            inv, \n",
    "                                                            lambda2, \n",
    "                                                            method=\"dSPM\", \n",
    "                                                            pick_ori=None)\n",
    "        # stc_vec     = mne.minimum_norm.apply_inverse_epochs(epochs_ar, \n",
    "        #                                                     inv, \n",
    "        #                                                     lambda2, \n",
    "        #                                                     method=\"dSPM\", \n",
    "        #                                                     pick_ori=\"vector\")\n",
    "        src = inv[\"src\"]\n",
    "        # brain = stc_vec[0].plot(\n",
    "        #     hemi=\"both\",\n",
    "        #     src=inv[\"src\"],\n",
    "        #     views=\"coronal\",\n",
    "        #     # initial_time=initial_time,\n",
    "        #     subjects_dir=subjects_dir,\n",
    "        #     brain_kwargs=dict(silhouette=True),\n",
    "        #     smoothing_steps=7,\n",
    "        # )\n",
    "        # Get labels for FreeSurfer 'aparc' cortical parcellation with 34 labels/hemi\n",
    "        labels_parc = mne.read_labels_from_annot(subject, parc=parc, subjects_dir=subjects_dir)\n",
    "        labels = mne.read_labels_from_annot(\"sample\", parc=\"aparc\", subjects_dir=subjects_dir)\n",
    "        \n",
    "        label_ts = [mne.extract_label_time_course(\n",
    "            stc[i], labels_parc, src, mode=\"mean\", allow_empty=True\n",
    "        ) for i in range(len(stc))]\n",
    "        \n",
    "        np.save(f\"{f.replace('_meg.ds', 'label_ts.npy')}\", label_ts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db56c9f5-ad36-402c-b407-f61388fa69aa",
   "metadata": {},
   "source": [
    "At last! Let's extract some networks. Note here that we use the \"coherence\" method which, if we were using EEG, would be inappropriate due to volume conduction effects. For MEG, it is okay. We iterate over the previously-saved .npy source space timeseries data and save a series of matrices. We also create a dataset linked up to the clinical data from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaae0010-b7a8-4ee3-8168-b1c729a4ac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Connectivity analysis\n",
    "\n",
    "# Network analysis\n",
    "from mne_connectivity import spectral_connectivity_epochs\n",
    "from mne_connectivity.viz import plot_connectivity_circle\n",
    "from mne.viz import circular_layout\n",
    "from itertools import product\n",
    "\n",
    "data_path = mne.datasets.sample.data_path()\n",
    "subject = \"sample\"\n",
    "data_dir = data_path / \"MEG\" / subject\n",
    "subjects_dir = data_path / \"subjects\"\n",
    "type_ = \"source\"\n",
    "sfreq = 250\n",
    "bands = {\"theta\": (4, 8),\n",
    "         \"alpha\": (8, 12),\n",
    "         \"beta\": (12, 25),\n",
    "         \"broad\": (1, 100)}\n",
    "\n",
    "# pop = \"Typically-Developing\"\n",
    "\n",
    "for type_ in [\"source\"]:#, \"sensor\"]:\n",
    "    # break\n",
    "    for band in bands:\n",
    "        fmin = bands[band][0]\n",
    "        fmax = bands[band][1]\n",
    "        \n",
    "        class_labels = []\n",
    "        conn_raveled_list = []\n",
    "        for idx_f, f in enumerate(all_files):\n",
    "            pop = f.split(\"/\")[-2].split(\"_\")[-1]\n",
    "            \n",
    "            if type_==\"source\":\n",
    "                if os.path.isfile(f\"{f.replace('_meg.ds', 'label_ts.npy')}\")==False:\n",
    "                    continue\n",
    "                else:\n",
    "                    label_ts = list(np.load(f\"{f.replace('_meg.ds', 'label_ts.npy')}\"))\n",
    "            else:\n",
    "                if os.path.isfile(f\"{f.replace('_meg.ds', '_ica15_reconst_meg_ar_epo.fif')}\")==False:\n",
    "                    continue\n",
    "                else:\n",
    "                    label_ts = mne.read_epochs(f\"{f.replace('_meg.ds', '_ica15_reconst_meg_ar_epo.fif')}\")\n",
    "                    label_ts = label_ts.pick(\"mag\")\n",
    "            class_labels.append(1)\n",
    "            \n",
    "            conn_data_list = []\n",
    "            for l in label_ts:\n",
    "                conn = spectral_connectivity_epochs([l], method='coh', sfreq=sfreq, mode='multitaper', \n",
    "                                                    fmin=fmin, fmax=fmax, faverage=False, tmin=None, tmax=None, \n",
    "                                                    n_jobs=-1)\n",
    "                conn_data = conn.get_data(output=\"dense\").mean(axis=2)[:, :, np.newaxis]#[:, :, 0]#\n",
    "                # conn_raw = conn.get_data(output=\"dense\")\n",
    "                conn_data_list.append(conn_data)\n",
    "            \n",
    "            conn_raw = np.concatenate(conn_data_list, axis=2)\n",
    "            \n",
    "            if type_==\"source\":\n",
    "                fpath = f\"/home/coveneuro-leif/Downloads/dl_project_{band}/{f.split('/')[-1].split('.')[0]}_{band}.npy\"\n",
    "                np.save(fpath, conn_raw)\n",
    "                if pop==\"Typically-Developing\":\n",
    "                    class_labels.append(1)\n",
    "                elif pop==\"ADHD\":\n",
    "                    class_labels.append(2)\n",
    "                elif pop==\"ASD\":\n",
    "                    class_labels.append(3)\n",
    "\n",
    "            # First, we reorder the labels based on their location in the left hemi\n",
    "            labels = mne.read_labels_from_annot(\"sample\", parc=\"aparc\", subjects_dir=subjects_dir)\n",
    "            labels_vol = [\n",
    "                \"Left-Amygdala\",\n",
    "                \"Left-Thalamus-Proper\",\n",
    "                \"Left-Cerebellum-Cortex\",\n",
    "                \"Brain-Stem\",\n",
    "                \"Right-Amygdala\",\n",
    "                \"Right-Thalamus-Proper\",\n",
    "                \"Right-Cerebellum-Cortex\",\n",
    "            ]\n",
    "            \n",
    "            label_names = [label.name for label in labels] + labels_vol\n",
    "            lh_labels = [name for name in label_names if name.endswith(\"lh\")]\n",
    "            \n",
    "            # Get the y-location of the label\n",
    "            label_ypos = list()\n",
    "            for name in lh_labels:\n",
    "                idx = label_names.index(name)\n",
    "                ypos = np.mean(labels[idx].pos[:, 1])\n",
    "                label_ypos.append(ypos)\n",
    "            \n",
    "            # Reorder the labels based on their location\n",
    "            lh_labels = [label for (yp, label) in sorted(zip(label_ypos, lh_labels))]\n",
    "            rh_labels = [label[:-2] + \"rh\" for label in lh_labels]\n",
    "            \n",
    "            # Save the plot order and create a circular layout\n",
    "            node_order = list()\n",
    "            node_order.extend(lh_labels[::-1])  # reverse the order\n",
    "            node_order.extend(rh_labels)\n",
    "            node_order.extend(labels_vol)\n",
    "            \n",
    "            node_angles = circular_layout(label_names, node_order, start_pos=90, group_boundaries=[0, len(label_names) / 2])\n",
    "            \n",
    "            ''' Plotting connectivity circle(s)\n",
    "            # Plot the graph using node colors from the FreeSurfer parcellation. We only\n",
    "            # show the 300 strongest connections.\n",
    "            fig, ax = plt.subplots(figsize=(8, 8), facecolor=\"black\", subplot_kw=dict(polar=True))\n",
    "            plot_connectivity_circle(\n",
    "                conn_data,\n",
    "                label_names,\n",
    "                n_lines=300,\n",
    "                node_angles=node_angles,\n",
    "                # node_colors=label_colors,\n",
    "                title=\"All-to-All Connectivity left-Auditory \" \"Condition (PLI)\",\n",
    "                ax=ax,\n",
    "            )\n",
    "            fig.tight_layout()\n",
    "            '''\n",
    "            \n",
    "            if type_==\"source\":\n",
    "                conn_df = pd.DataFrame(data = conn_raw.mean(axis=2), \n",
    "                                       index = lh_labels+rh_labels+labels_vol, \n",
    "                                       columns = lh_labels+rh_labels+labels_vol)\n",
    "            else:\n",
    "                conn_df = pd.DataFrame(data = conn_data, \n",
    "                                       index = label_ts.info['ch_names'], \n",
    "                                       columns = label_ts.info['ch_names'])\n",
    "            \n",
    "            pairs = [f\"{x}_{y}\" for x, y in list(product(conn_df.columns, conn_df.index))]\n",
    "            conn_raveled = pd.DataFrame(data = conn_df.values.ravel(),\n",
    "                                        index=pairs).T\n",
    "            conn_raveled.index = [\"_\".join(f.split(\"/\")[-1].split(\"_\")[:3])]\n",
    "            \n",
    "            conn_raveled_list.append(conn_raveled)\n",
    "        \n",
    "        conn_raveled_df = pd.concat(conn_raveled_list)\n",
    "        conn_raveled_df.to_csv(f\"{f.replace('_meg.ds', f'_conn_df_{type_}_{band}.csv')}\")\n",
    "        np.save(f\"/home/coveneuro-leif/Downloads/dl_project/labels_{pop}.npy\", class_labels, allow_pickle=True)\n",
    "        # plt.plot(conn_raveled_df.T)\n",
    "        \n",
    "        ''' # Misc/ICA etc.\n",
    "        ica.plot_components()\n",
    "        ica.plot_sources(filtered)\n",
    "        filtered.plot()\n",
    "        ica.plot_overlay(filtered, exclude=[0,1], picks=\"mag\")\n",
    "        ica.exclude = [0, 1]\n",
    "        reconst = filtered.copy()\n",
    "        ica.apply(reconst)\n",
    "        \n",
    "        # Comparing pre- vs post-ICA\n",
    "        filtered.plot()\n",
    "        reconst.plot()\n",
    "        '''\n",
    "        \n",
    "        ## %% Clinical - MEG tie up\n",
    "        clinical = pd.read_csv(\"/home/coveneuro-leif/clinical_SWAN.csv\")\n",
    "        clinical.index = clinical['subject_id']\n",
    "        # sfreq = epochs_ar.info['sfreq']\n",
    "        # final_dataset = pd.concat([clinical, conn_raveled_df])\n",
    "        final_dataset = pd.merge(left=clinical, right=conn_raveled_df,\n",
    "                                  left_index=True, right_index=True)\n",
    "        \n",
    "        final_dataset = final_dataset.loc[:, (final_dataset != 0).any(axis=0)]\n",
    "        final_dataset.to_csv(\"/\".join(all_files[0].split(\"/\")[:-1])+f\"/connectivity_{type_}_{band}_fixed.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec536048-5b65-43b8-a47e-f53db6eb04c8",
   "metadata": {},
   "source": [
    "Now, we do the same for fMRI. We will specifically use the ANTsPy libraries, as these are open and available for commercial use, as opposed to the libraries like NiPype which, while outstanding, rely on closed-source or copyleft (=your codebase is required to be open if you use their code) tools like FSL and SPM. For commercial entities, this is an important consideration and, given the work required to develop and tune a preprocessing workflow, it is better to build the best way, from the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819183f2-9953-49bf-a3c8-96ace4da718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "\n",
    "from nilearn.maskers import NiftiLabelsMasker\n",
    "from nilearn.maskers import NiftiMapsMasker\n",
    "import numpy as np\n",
    "import nilearn\n",
    "import seaborn as sns\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from sklearn.covariance import GraphicalLassoCV\n",
    "from nilearn.connectome import GroupSparseCovarianceCV\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn.datasets import MNI152_FILE_PATH\n",
    "from nilearn import plotting\n",
    "from nilearn import image\n",
    "from nilearn.image import index_img\n",
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "from nilearn.image import mean_img\n",
    "import pandas as pd\n",
    "import glob\n",
    "import ants\n",
    "import antspynet\n",
    "import time\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import ants\n",
    "from nilearn.masking import compute_epi_mask, apply_mask\n",
    "from nilearn.plotting import plot_epi, plot_roi\n",
    "from nilearn.image.image import mean_img\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn.input_data import NiftiLabelsMasker\n",
    "from nilearn import datasets\n",
    "from nilearn.image import clean_img\n",
    "from nilearn import image\n",
    "\n",
    "pops = [\"ADHD\", \"ASD\", \"Typically-Developing\"]\n",
    "\n",
    "pop = pops[0]\n",
    "MR_dfs = []\n",
    "for pop in pops:\n",
    "    anat = [i.replace(\"\\\\\", \"/\") for i in glob.glob(f\"/home/coveneuro-leif/Downloads/Imaging_MR-T1_{pop}/*.nii.gz\")]\n",
    "    anat_df = pd.DataFrame(data = np.c_[[\"_\".join(i.split(\"/\")[-1].split(\"_\")[0:4]) for i in anat], anat], columns = [\"subid\", \"path\"])\n",
    "    fxal = [i.replace(\"\\\\\", \"/\") for i in glob.glob(f\"/home/coveneuro-leif/Downloads/Imaging_fMRI-RestingState_{pop}/*.nii\")]\n",
    "    fxal_df = pd.DataFrame(data = np.c_[[\"_\".join(i.split(\"/\")[-1].split(\"_\")[0:4]) for i in fxal], fxal], columns = [\"subid\", \"path\"])\n",
    "    MR_merged = pd.merge(left=anat_df, right=fxal_df, left_on=\"subid\", right_on=\"subid\")\n",
    "    MR_merged.columns = [\"subid\", \"path_structural\", \"path_functional\"]\n",
    "    MR_dfs.append(MR_merged)\n",
    "\n",
    "MR_merged_final = pd.concat(MR_dfs)\n",
    "T1_files = MR_merged_final[\"path_structural\"].values\n",
    "fmri_files = MR_merged_final[\"path_functional\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0058dbfe-8f44-458e-8853-4175e7391cc3",
   "metadata": {},
   "source": [
    "Time for fMRI preprocessing. This is made fairly painless using ANTsPy. We strip the skull using an AI method from ANTsPyNet, then correct for motion, and register the functional image to the corresponding anatomical image and then to a template to ensure standardization. We use publicly available developmental fMRI templates, as childrens' heads are clearly different from adults' heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4c8cc7-0e33-4503-9811-94e099e5066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_file = fmri_files[1]  # Path to the fMRI file\n",
    "\n",
    "# motions = {}\n",
    "bads = []\n",
    "for ix_f, fmri_file in enumerate(fmri_files):\n",
    "    # break\n",
    "    # try:\n",
    "    print(f\"\\n\\n\\n {ix_f}\\n\\n\\n\")\n",
    "    st = time.time()\n",
    "    \n",
    "    # Get matching T1/fMRI files\n",
    "    fmri_subid = \"_\".join(fmri_file.split(\"/\")[-1].split(\"_\")[:4])\n",
    "    t1_file = T1_files[np.array([fmri_subid in t for t in T1_files])][0]\n",
    "    pop = fmri_file.split(\"/\")[-2].split(\"_\")[-1]\n",
    "    print(f\"Sorted file IDs at {time.time()-st} sec\")\n",
    "    \n",
    "    # TODO SUPER HELPFUL\n",
    "    # https://cdn.jamanetwork.com/ama/content_public/journal/jamanetworkopen/939092/zoi230095supp1_prod_1678113687.23714.pdf?Expires=1733297702&Signature=zr4d70M4ydKr5cOcCCR3DfGfrEaVRNu7Rw9crO5YwDeyMurZeptnWwc-ADevB0BI~g4bVdfPwXt~rZHbH7deS5-2FCk2JcyRagma~CBQybTjKz1T-Q6ALWUGJKtLSoDbV5SPY0-qLcSmbUoxFcF5-Q8ImRwLHrVSYQ9YcSH27v1LtxlvhUGxVdSIpxJXIoIou0U8rJ0nIww33lyNahEdU8E1P7mVAt2bpEo8pk8wvVxxxBVamrDcTui1oNH7mPYLx6P5X9O7MzEkoM1hYX3n4g99fi6UIF-S~77hTgwwtN6OpsXcRJg6MdrkYqxCJXw-aeDy8-uZVl006s5AEzZ4-A__&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA\n",
    "    \n",
    "    ## Load files\n",
    "    fmri = ants.image_read(fmri_file)\n",
    "    fmri_nib = nib.load(fmri_file)  # Load the fMRI image using nibabel for skull stripping\n",
    "    t1 = ants.image_read(t1_file)\n",
    "    # ants.plot(t1)\n",
    "    print(f\"Read files at {time.time()-st} sec\")\n",
    "    \n",
    "    ## Strip skull\n",
    "    fmri_mask = compute_epi_mask(fmri_nib)\n",
    "    fmri_data = fmri_nib.get_fdata()\n",
    "    mask_data = fmri_mask.get_fdata()\n",
    "    skull_stripped_data = fmri_data*mask_data[..., None]\n",
    "    skull_stripped_img = nib.Nifti1Image(skull_stripped_data, fmri_nib.affine, fmri_nib.header)\n",
    "    fmri_brain_path = 'skull_stripped_fmri.nii.gz'\n",
    "    nib.save(skull_stripped_img, fmri_brain_path)\n",
    "    fmri_brain = ants.image_read(fmri_brain_path)\n",
    "    # ants.plot(ants.slice_image(fmri_brain, axis=3, idx=0))\n",
    "    \n",
    "    seg = antspynet.brain_extraction(t1, modality=\"t1\", verbose=True)\n",
    "    # ants.plot(t1, overlay=seg, overlay_alpha=0.5)\n",
    "    t1_brain = t1*seg\n",
    "    print(f\"Stripped skulls at {time.time()-st} sec\")\n",
    "    \n",
    "    ## Motion correction \n",
    "    ref_vol = ants.slice_image(fmri_brain, axis=3, idx=fmri_brain.shape[3]//2)\n",
    "    fmri_corrected = ants.motion_correction(fmri_brain, target_image=ref_vol)\n",
    "    motion_params = fmri_corrected['motion_parameters']\n",
    "        \n",
    "    # with open(f\"{fmri_subid}_motion_params.pkl\", \"rb\") as file:\n",
    "    #     motpars = pickle.load(file)\n",
    "    print(f\"Motion corrected at {time.time()-st} sec\")\n",
    "    \n",
    "    ## Registration: T1<>Template\n",
    "    template_path = \"/home/coveneuro-leif/Downloads/pediatric_templates/nihpd_asym_04.5-18.5_t1w.nii\"\n",
    "    mni_template = ants.image_read(template_path)\n",
    "    seg_template = antspynet.brain_extraction(mni_template, modality=\"t1\", verbose=True)\n",
    "    mni_template = mni_template*seg_template\n",
    "    t1_to_mni = ants.registration(fixed=mni_template, moving=t1_brain, type_of_transform=\"Affine\")\n",
    "    print(f\"Registered T1-template at {time.time()-st} sec\")\n",
    "    \n",
    "    ## Registration: Functional<>Anatomical\n",
    "    # TODO following line: select middle volume, apply across all (see CGPT)\n",
    "    middle_idx = fmri_corrected['motion_corrected'].shape[-1] // 2\n",
    "    fmri_single_vol = ants.slice_image(fmri_corrected['motion_corrected'], axis=3, idx=middle_idx)\n",
    "    # ants.plot(fmri_single_vol)\n",
    "    fmri_to_t1 = ants.registration(fixed=t1_brain, moving=fmri_single_vol, type_of_transform=\"Affine\") # \"SyN\"\n",
    "    coregistered_bold = fmri_to_t1['warpedmovout']\n",
    "    print(f\"Registered fMRI-T1 at {time.time()-st} sec\")\n",
    "\n",
    "    t1_resamp = ants.resample_image(t1_brain, (3.0, 3.0, 3.0), use_voxels=False)\n",
    "    fmri_aligned = ants.apply_transforms(fixed=t1_resamp, \n",
    "                                              moving=fmri_corrected['motion_corrected'], \n",
    "                                              transformlist=fmri_to_t1['fwdtransforms']+t1_to_mni['fwdtransforms'], \n",
    "                                              interpolator='linear',\n",
    "                                              imagetype=3)\n",
    "\n",
    "    ants.image_write(fmri_aligned, f\"{fmri_subid}_{pop}_fmri_aligned.nii.gz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428eabbe-9612-46ff-80f8-3d3c29a5cb9c",
   "metadata": {},
   "source": [
    "We perform final cleaning as necessary prior to extracting the timeseries of brain activity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba36785-e808-4676-b77d-9dfddbea5ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load an atlas with labeled regions (e.g., Harvard-Oxford atlas)\n",
    "atlas = datasets.fetch_atlas_harvard_oxford('cort-maxprob-thr25-2mm')\n",
    "atlas_filename = atlas.maps  # Path to the atlas labels\n",
    "labels = atlas.labels        # Region names\n",
    "\n",
    "bads = []\n",
    "# ix_f = 97+125 had a compression error (due to memory issues before?)\n",
    "# ix_f = 97+126+7 had a compression error (due to memory issues before?)\n",
    "for ix_f, fmri_file in enumerate(fmri_files):\n",
    "    print(f\"\\n {ix_f}\")\n",
    "    st = time.time()\n",
    "    \n",
    "    # Get matching T1/fMRI files\n",
    "    fmri_subid = \"_\".join(fmri_file.split(\"/\")[-1].split(\"_\")[:4])\n",
    "    t1_file = T1_files[np.array([fmri_subid in t for t in T1_files])][0]\n",
    "    pop = fmri_file.split(\"/\")[-2].split(\"_\")[-1]\n",
    "    print(f\"Sorted file IDs at {time.time()-st} sec\")\n",
    "    \n",
    "    fmri_aligned_img = image.load_img(f\"{fmri_subid}_{pop}_fmri_aligned.nii.gz\")\n",
    "    # fmri_aligned_img = image.load_img(\"/home/coveneuro-leif/PND03_HSC_0019_01_ADHD_fmri_aligned.nii.gz\")\n",
    "    try:\n",
    "        if os.path.isfile(f\"{fmri_subid}_{pop}_fmri_aligned_cleaned.nii.gz\")==False:\n",
    "            if fmri_aligned_img.shape[-1]<100:\n",
    "                continue\n",
    "            fmri_cleaned = clean_img(\n",
    "                fmri_aligned_img,\n",
    "                detrend=True,             # Remove slow drifts\n",
    "                standardize=True,         # Standardize each voxel's time series to z-scores\n",
    "                low_pass=0.1,             # Low-pass filter (frequency in Hz, adjust as needed)\n",
    "                high_pass=0.008,          # High-pass filter (frequency in Hz, adjust as needed)\n",
    "                t_r=1.5                   # Repetition time (TR) in seconds, adjust to match your data\n",
    "            )\n",
    "            nib.save(fmri_cleaned, f\"{fmri_subid}_{pop}_fmri_aligned_cleaned.nii.gz\")\n",
    "            # nib.save(fmri_cleaned, \"/home/coveneuro-leif/PND03_HBK_0265_03_ASD_fmri_aligned_cleaned.nii.gz\")\n",
    "        else:\n",
    "            print(\"Sample already cleaned\")\n",
    "            pass\n",
    "        \n",
    "        # Load preprocessed fMRI image\n",
    "        fmri_cleaned = nib.load(f\"{fmri_subid}_{pop}_fmri_aligned_cleaned.nii.gz\")\n",
    "        \n",
    "        # Set up the NiftiLabelsMasker\n",
    "        masker = NiftiLabelsMasker(labels_img=atlas_filename, standardize=True, detrend=True)\n",
    "        \n",
    "        # Extract timeseries for each region\n",
    "        timeseries_data = masker.fit_transform(fmri_cleaned)\n",
    "        np.save(f\"{fmri_subid}_{pop}_fmri_aligned_cleaned_timeseries.npy\", timeseries_data)\n",
    "        print(\"Timeseries extracted\\n\")\n",
    "        # timeseries_data = np.load(f\"{fmri_subid}_{pop}_fmri_aligned_cleaned_timeseries.npy\")\n",
    "    except:\n",
    "        bads.append(f\"{fmri_subid}_{pop}_fmri_aligned_cleaned.nii.gz\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89453221-f801-482e-b52d-7b9c2eeb6414",
   "metadata": {},
   "source": [
    "At long last, we can extract the connectivity matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af85424-5cea-4a58-b58f-421529d9b253",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "window_size = 10\n",
    "step_size = 1\n",
    "\n",
    "def get_windows(window_size, step_size, timeseries_data):\n",
    "    num_windows = (timeseries_data.shape[0]-window_size)//(step_size+1)\n",
    "    windows = np.array([timeseries_data[(i*step_size):(i*step_size+window_size), :] for i in range(num_windows)])\n",
    "    return windows\n",
    "\n",
    "# TODO use \"multi\" to re-save arrays\n",
    "kind = \"multi\" # single, multi\n",
    "raveled_mats = []\n",
    "pops = []\n",
    "subids = []\n",
    "\n",
    "for ix_f, fmri_file in enumerate(fmri_files):\n",
    "    # break\n",
    "    try:\n",
    "        print(f\"\\n\\n\\n {ix_f}\\n\\n\\n\")\n",
    "        st = time.time()\n",
    "        \n",
    "        # Get matching T1/fMRI files\n",
    "        fmri_subid = \"_\".join(fmri_file.split(\"/\")[-1].split(\"_\")[:4])\n",
    "        t1_file = T1_files[np.array([fmri_subid in t for t in T1_files])][0]\n",
    "        pop = fmri_file.split(\"/\")[-2].split(\"_\")[-1]\n",
    "        print(f\"Sorted file IDs at {time.time()-st} sec\")\n",
    "        \n",
    "        timeseries_data = np.load(f\"/home/coveneuro-leif/{fmri_subid}_{pop}_fmri_aligned_cleaned_timeseries.npy\")\n",
    "        connectivity_measure = ConnectivityMeasure(kind=\"correlation\")\n",
    "        \n",
    "        if kind==\"single\":\n",
    "            connectome_matrix = connectivity_measure.fit_transform([timeseries_data])[0]\n",
    "            np.fill_diagonal(connectome_matrix, 0)\n",
    "            raveled = np.tril(connectome_matrix).ravel()\n",
    "            # plotting.plot_matrix(correlation_matrix, labels=labels, colorbar=True, vmax=0.8, vmin=-0.8)\n",
    "            \n",
    "            if raveled.shape[0]==2304: # To avoid dimension issues - we only lose 9/474\n",
    "                raveled_mats.append(raveled)\n",
    "                pops.append(pop)\n",
    "                subids.append(fmri_subid)\n",
    "            \n",
    "        else:\n",
    "            windows = get_windows(window_size, step_size, timeseries_data)\n",
    "            connectome_matrix = connectivity_measure.fit_transform(windows)\n",
    "            connectome_matrix = connectome_matrix.transpose(1,2,0)\n",
    "            \n",
    "            if (connectome_matrix.shape[0]==48) & (connectome_matrix.shape[1]==48) & (connectome_matrix.shape[2]>=40):\n",
    "                np.save(f\"/home/coveneuro-leif/Downloads/dl_project_fmri/{fmri_subid}_{pop}_fmri_aligned_cleaned_timeseries_connMat.npy\", connectome_matrix)\n",
    "            else:\n",
    "                pass\n",
    "    except:\n",
    "        print(ix_f)\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa955ddc-45f4-481e-b0e0-c2a2eb387b44",
   "metadata": {},
   "source": [
    "That's it for preprocessing fMRI! We are now ready to start having some data modeling fun."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8d7dc1-05a7-4b69-a1ee-3d0098ca4551",
   "metadata": {
    "tags": []
   },
   "source": [
    " # Analyzing the data\n",
    " \n",
    "In this part, we will focus on building the models and analyzing the connectivity matrices that we generated above after all of our preprocessing efforts. First, we'll import our relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dce06b-1f03-4187-a4d2-f77d1b1c90d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from cca_zoo.model_selection import GridSearchCV\n",
    "from cca_zoo.linear import SCCA_IPLS\n",
    "import pingouin as pg\n",
    "from scipy.stats import spearmanr\n",
    "import mne\n",
    "from sklearn.decomposition import SparsePCA, PCA, KernelPCA\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['svg.fonttype']=\"none\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc23761-f4f5-48e8-a342-c1f6ce714e15",
   "metadata": {},
   "source": [
    "Let's define some custom functions to help us out. \n",
    "\n",
    "1. plot_kde() is a helper to plot kernel density estimates (KDEs) over top of data points, to help with visualization.\n",
    "2. scorer() is a custom scoring function that we'll be using later during mode development.\n",
    "\n",
    "Also, we set the torch and numpy seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bdea89-46a9-4e42-8285-432cfa805c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kde(V, ax, colour = 'k', label=None, limits=2, n_levels=2):\n",
    "    # V is a matrix where columns = variables and rows = samples\n",
    "    # ax is the axis to attach the plot to\n",
    "    from scipy.stats import gaussian_kde\n",
    "\n",
    "    kernel = gaussian_kde(V.T)\n",
    "    xmin, ymin, xmax, ymax = np.r_[V.min(axis = 0)-limits, V.max(axis = 0)+limits]\n",
    "    \n",
    "    # Peform the kernel density estimate\n",
    "    xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "    positions = np.vstack([xx.ravel(), yy.ravel()])\n",
    "    f = np.reshape(kernel(positions).T, xx.shape)\n",
    "    \n",
    "    # cfset = ax.contourf(xx, yy, f, cmap='Blues')\n",
    "    ax.contour(xx, yy, f, colors=colour, levels = n_levels, linewidths = 2, label=label)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "# Custom scoring function\n",
    "def scorer(estimator, X):\n",
    "    dim_corrs = estimator.score(X)\n",
    "    return dim_corrs.mean()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4247e1da-cdaa-44c4-8a3d-84862cf79046",
   "metadata": {},
   "source": [
    "Let's define the model and dataloader for the fMRI data first. The convolutional autoencoder takes in whole 3D matrices from each individual, which are accessed using the dataloader (the first 40 time slices, to ensure consistency across individuals), and then squashes them down to a 2D latent space of shape (batch_size, latent_dim). Once the model is trained, we can run just the encoder to extract latent codes for new individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c364e7c6-8ad3-4e32-a76b-f455a5238169",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Conv3DAutoencoderFMRI(nn.Module):\n",
    "    def __init__(self, latent_dim=2):\n",
    "        super(Conv3DAutoencoderFMRI, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder\n",
    "        # x = inputs.to(\"cpu\")\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1),  # Output: (16, 75, 75, 10)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=2),  # Output: (16, 37, 37, 5)\n",
    "\n",
    "            nn.Conv3d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),  # Output: (32, 37, 37, 5)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=2)  # Output: (32, 18, 18, 2)\n",
    "        )\n",
    "        # 32*12*12*10\n",
    "        ldim = 32*12*12*10\n",
    "        self.fc_mu = nn.Linear(ldim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(ldim, latent_dim)\n",
    "        self.fc_decode = nn.Linear(latent_dim, ldim)\n",
    "        \n",
    "        # Decoder\n",
    "        # x1=x\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose3d(in_channels=32, out_channels=16, kernel_size=2, stride=2),  # Output: (16, 36, 36, 4)\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.ConvTranspose3d(in_channels=16, out_channels=8, kernel_size=2, stride=2),  # Output: (8, 72, 72, 8)\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.ConvTranspose3d(in_channels=8, out_channels=1, kernel_size=(3,3,3), stride=(1,1,1), padding=(1,1,1)),#(x1).shape,  # Output: (1, 75, 75, 10)\n",
    "            nn.Sigmoid()  # Output values between 0 and 1\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)  # Standard deviation\n",
    "        eps = torch.randn_like(std)  # Random noise\n",
    "        return mu + eps * std  # Reparameterization trick\n",
    "\n",
    "    def decode(self, z):\n",
    "        x = self.fc_decode(z)\n",
    "        x = x.view(-1, 32, 12, 12, 10)  # Reshape to (32, 18, 18, 2)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z)\n",
    "        return recon, mu, logvar\n",
    "    \n",
    "class Custom3DDatasetFMRI(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (np.ndarray or list): List or numpy array of shape (num_samples, 75, 75, 10).\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.file_list = [f for f in os.listdir(data_dir) if f.endswith('.npy')] \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.data_dir, self.file_list[idx])\n",
    "        sample = np.load(file_path)[:, :, :40]\n",
    "        sample = torch.tensor(sample, dtype=torch.float32).unsqueeze(0)  # Shape: (1, 75, 75, 10)\n",
    "        \n",
    "        return sample, file_path\n",
    "# def train_autoencoder(model, dataloader, criterion, optimizer, num_epochs=10):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7259986-9adb-4813-b1ff-54c284e5e393",
   "metadata": {},
   "source": [
    "Time to train the model (already!) This is the fMRI model. It will pull from the saved matrices that we generated previously, bring them in through our dataloader class (above), and fit the AE model, saving every 100th epoch to file for reproducibility. We also loop over multiple latent dimension sizes so that they can be inspected later on if we wish. Finally, we specify to run the models using CUDA if available, as it speeds up the computation manifold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a1d4cc-681e-4615-a693-9d8b5a29b373",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "data_dir = \"/home/coveneuro-leif/Downloads/dl_project_fmri/\"\n",
    "dataset = Custom3DDatasetFMRI(data_dir)\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 16\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "for latent_dim in [4, 8, 16, 32, 64]:\n",
    "    num_epochs = 1001 # 1001 so that the 1000th epoch is also saved. THANKS PYTHON\n",
    "    \n",
    "    # Instantiate the model\n",
    "    model = Conv3DAutoencoderFMRI(latent_dim=latent_dim)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Training setup\n",
    "    criterion = nn.MSELoss()  # Mean squared error loss for reconstruction\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)  # Adam optimizer\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in np.arange(1, num_epochs):\n",
    "        running_loss = 0.0\n",
    "    \n",
    "        for inputs, _ in dataloader:\n",
    "            # break\n",
    "            inputs = inputs+inputs.permute(0,1,3,2,4)\n",
    "            # Move data to GPU if available\n",
    "            inputs = inputs.to(device)\n",
    "            # break\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # Forward pass\n",
    "            outputs = model(inputs) # outputs[0].shape\n",
    "            \n",
    "            # Compute the loss (between the input and its reconstruction)\n",
    "            loss = criterion(outputs[0], inputs) + criterion(outputs[0].permute(0,1,3,2,4), inputs)\n",
    "            # loss = criterion(outputs[0], inputs)\n",
    "            \n",
    "            # Backpropagation and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            # Accumulate loss\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "        if epoch%100==0:\n",
    "            torch.save(model.state_dict(), f\"/home/coveneuro-leif/Results/3DCNN_AE_fMRI_{latent_dim}_{epoch}_all.pt\")\n",
    "        \n",
    "        # Print average loss for the epoch\n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        print(f'Epoch {epoch}/{num_epochs}, Loss: {epoch_loss:.6f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fd9645-184b-4ea2-b57d-7bb93e568e99",
   "metadata": {},
   "source": [
    "Next, let's get the anatomical labels for the fMRI dataset...this will be useful later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a1358a-a001-4dd1-b0b6-3c5293b9e7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "atlas = datasets.fetch_atlas_harvard_oxford('cort-maxprob-thr25-2mm')\n",
    "atlas_filename = atlas.maps  # Path to the atlas labels\n",
    "label_names = atlas.labels        # Region names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be5de5a-0604-4ad5-918a-6efa6cb07636",
   "metadata": {},
   "source": [
    "Next, we can start preparing to plot data from the learned latent space. Define some helpers, define some grouping variables, and then compute groupwise t-tests for identifying significant voxels. Let's grab the 1000th epoch and set the target latent_dim size to 4. Empirically, this combination leads to good visualizations and reconstruction performance without sacrificing separability between groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af41a896-951a-41f9-954c-82c93cfc0e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a cascade of latent traversal\n",
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)  # Standard deviation\n",
    "    eps = torch.randn_like(std)  # Random noise\n",
    "    return mu + eps * std  # Reparameterization trick\n",
    "\n",
    "# We saved these earlier during training...dealer's choice as to which can be used\n",
    "# Epochs must be in increments of 100 only \n",
    "latent_dim = 4\n",
    "epoch = 1000\n",
    "model = Conv3DAutoencoderFMRI(latent_dim=latent_dim)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(f\"/home/coveneuro-leif/Results/3DCNN_AE_fMRI_{latent_dim}_{epoch}_all.pt\"))\n",
    "\n",
    "model.eval()\n",
    "means = []\n",
    "vars_ = []\n",
    "names_data = []\n",
    "groups_data = []\n",
    "for data, name in dataloader:\n",
    "    data = torch.Tensor(data).to(device)  # Send data to GPU if available\n",
    "    out = model.encode(data)\n",
    "    means.append(out[0].cpu().detach().numpy())\n",
    "    vars_.append(out[1].cpu().detach().numpy())\n",
    "    names_data.extend(name)\n",
    "    \n",
    "    for n in name:\n",
    "        if \"Typically-Developing\" in n:\n",
    "            groups_data.append(\"Typically-Developing\")\n",
    "        elif \"ASD\" in n:\n",
    "            groups_data.append(\"ASD\")\n",
    "        elif \"ADHD\" in n:\n",
    "            groups_data.append(\"ADHD\")\n",
    "        else:\n",
    "            groups_data.append(\"NA\")\n",
    "\n",
    "clinical = pd.read_csv(\"/home/coveneuro-leif/clinical_SWAN.csv\")\n",
    "names_data = np.array(names_data)\n",
    "# all_files = glob.glob(\"/home/coveneuro-leif/Downloads/ae_data/*.npy\")\n",
    "all_names = [\"_\".join(a.split(\"/\")[-1].split(\".\")[0].split(\"_\")[:3]) for a in names_data]\n",
    "all_means = np.concatenate(means)\n",
    "all_groups = np.array(groups_data)\n",
    "\n",
    "# Numerical group names so that plotting can be done more easily\n",
    "all_groups_num = []\n",
    "for a in all_groups:\n",
    "    if \"Typically-Developing\" in a:\n",
    "        all_groups_num.append(1)\n",
    "    if \"ADHD\" in a:\n",
    "        all_groups_num.append(2)\n",
    "    if \"ASD\" in a:\n",
    "        all_groups_num.append(3)\n",
    "    if \"NA\" in a:\n",
    "        all_groups_num.append(np.nan)\n",
    "        \n",
    "all_groups_num = np.array(all_groups_num)        \n",
    "all_means_df = pd.DataFrame(data=all_means.astype(float), index=all_names, columns=[f\"lat{i}\" for i in range(all_means.shape[1])])\n",
    "\n",
    "# TODO use this to sort latents for plotting timeseries\n",
    "from scipy.stats import ttest_ind\n",
    "stats_results = []\n",
    "for i in range(latent_dim):\n",
    "    var = f\"lat{i}\"\n",
    "    # plt.figure()\n",
    "    # plt.boxplot(positions=[0], x=all_means_df[var][all_groups_num==1])\n",
    "    # plt.boxplot(positions=[1], x=all_means_df[var][all_groups_num==2])\n",
    "    # plt.boxplot(positions=[2], x=all_means_df[var][all_groups_num==3])\n",
    "    stat_1_2, pval_1_2 = ttest_ind(all_means_df[var][all_groups_num==1], all_means_df[var][all_groups_num==2])\n",
    "    stat_2_3, pval_2_3 = ttest_ind(all_means_df[var][all_groups_num==2], all_means_df[var][all_groups_num==3])\n",
    "    stat_1_3, pval_1_3 = ttest_ind(all_means_df[var][all_groups_num==1], all_means_df[var][all_groups_num==3])\n",
    "    stats_results.append((pval_1_2, pval_2_3, pval_1_3, stat_1_2, stat_2_3, stat_1_3))\n",
    "stats_results_df = pd.DataFrame(data=stats_results, \n",
    "                                columns=[\"p-TD_ADHD\", \"p-ADHD_ASD\", \"p-TD_ASD\", \"t-TD_ADHD\", \"t-ADHD_ASD\", \"t-TD_ASD\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b548996-b961-41d3-8afb-1b75e69147ab",
   "metadata": {},
   "source": [
    "Next, we will traverse the latent space to see how the connectivity matrices change with a change in the latent variables - the most significant latent dimensions also correspond to notable changes in the patterns of the matrices!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e718e9-c6d7-4ea1-8125-9ed85906ba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, logvar = model.encode(data)\n",
    "z = reparameterize(mu, logvar)\n",
    "pred_original = model.decode(z).cpu().detach().numpy()\n",
    "\n",
    "# Perform latent space traversal in statistically-identified ROIs \n",
    "num_traversal = 5\n",
    "perturb_max = 2\n",
    "latent_perturb = 1\n",
    "\n",
    "fig, ax = plt.subplots(1, num_traversal) # Top row: ax0 traversal, Bottom row: ax1 traversal\n",
    "plt.subplots_adjust(wspace=0.5, left=0.05, right=0.99)\n",
    "add_vals = np.linspace(-perturb_max, perturb_max, num_traversal)\n",
    "\n",
    "for i in range(num_traversal):\n",
    "    test_arr = np.zeros(all_means.shape[1])\n",
    "    test_arr[latent_perturb] = add_vals[i]\n",
    "    \n",
    "    # pred_ax = model.decode(z+torch.Tensor(test_arr).to(device)).cpu().detach().numpy()\n",
    "    pred_ax = model.decode(z+torch.Tensor(test_arr).to(device)).cpu().detach().numpy() - pred_original\n",
    "    \n",
    "    # Set aside highest and lowest traversale for finding the nodes that change the most (NEXT STAGE)\n",
    "    if i==0:\n",
    "        mat_neg = pred_ax\n",
    "    elif i==(num_traversal-1):\n",
    "        mat_pos = pred_ax\n",
    "        \n",
    "    ax[i].imshow(pred_ax.squeeze()[0][:, :, 0], vmax=0.5, vmin=-0.5, cmap=\"jet\")\n",
    "    ax[i].set_xticks(np.arange(len(label_names)), label_names, rotation=90, fontsize=4)\n",
    "    ax[i].set_yticks(np.arange(len(label_names)), label_names, rotation=00, fontsize=4)\n",
    "    \n",
    "main_nodes = mat_pos - mat_neg\n",
    "main_nodes = main_nodes.squeeze()[0][:, :, 0]\n",
    "plt.imshow(main_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2388d6bc-da93-4877-8f71-e6f1dd15a532",
   "metadata": {},
   "source": [
    "Let's plot an example of a reconstructed connectivity matrix vs a corresponding original. Note that the overall patterns are pretty close! Although in this specific example, the symmetry is imperfect. In MEG, the performance tends to be better. Also, fMRI with higher latent dimension counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffef18e2-89ab-4cab-aa6d-696a8786613d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# And now the magic - grab the indices of the highest-magnitude differences as you\n",
    "# traverse your statistically-defined latent of interest\n",
    "# TODO n.b. focus on perhaps statistically identifying the best candidate(s) for this step, \n",
    "# rather than relying solely on magnitude?\n",
    "max_r, max_c = np.unravel_index(np.argmax(np.abs(main_nodes)), main_nodes.shape)\n",
    "\n",
    "# Plot reconstructed connectivity matrices\n",
    "predicted = model(data)[0]\n",
    "predicted = predicted.cpu().detach().numpy()\n",
    "data_for_viz = data\n",
    "data_for_viz = data_for_viz+data_for_viz.permute(0,1,3,2,4)\n",
    "data_for_viz = data_for_viz.cpu().detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "plt.subplots_adjust(left=0.10, right=0.99, top=0.95, bottom=0.20, wspace=0.5)\n",
    "ax[0].set_title(\"Predicted\")\n",
    "ax[1].set_title(\"Original\")\n",
    "ax[0].imshow(predicted.squeeze()[0][:, :, 0], vmin=0, vmax=1)\n",
    "ax[1].imshow(data_for_viz.squeeze()[0][:, :, 0], vmin=0, vmax=1)\n",
    "ax[0].set_xticks(np.arange(len(label_names)), label_names, rotation=90, fontsize=8)\n",
    "ax[0].set_yticks(np.arange(len(label_names)), label_names, rotation=00, fontsize=8)\n",
    "ax[1].set_xticks(np.arange(len(label_names)), label_names, rotation=90, fontsize=8)\n",
    "ax[1].set_yticks(np.arange(len(label_names)), label_names, rotation=00, fontsize=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a172a8bf-ac35-44fd-9449-9a2851ab33d7",
   "metadata": {},
   "source": [
    "Because these are 3D volumes that are being reconstructed, we can also plot the the estimated/reconstructed timeseries derived from individual voxels! Pretty cool. Let's plot one of the most significantly varying ones from before...\n",
    "\n",
    "You will also note immediately that the VAE acts as an adaptive lowpass filter on the data. This could be useful as a preprocessing step for neurophysiological/neuroimaging timeseries data at scale (i.e., across many channels at once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef47167-8404-44a0-9f82-f8567ea0a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot timeseries of reconstructed 3D volumes\n",
    "fig, ax = plt.subplots(1, num_traversal) # Top row: ax0 traversal, Bottom row: ax1 traversal\n",
    "row = max_r\n",
    "col = max_c\n",
    "row_name = label_names[row]\n",
    "col_name = label_names[col]\n",
    "ts_combined_title = f\"Connectivity: {row_name}  {col_name}\"\n",
    "plt.suptitle(ts_combined_title)\n",
    "\n",
    "for i in range(num_traversal):\n",
    "    test_arr = np.zeros(all_means.shape[1])\n",
    "    test_arr[latent_perturb] = add_vals[i]\n",
    "    \n",
    "    # Extract most significant latent timeseries\n",
    "    pred_ax = model.decode(z+torch.Tensor(test_arr).to(device)).cpu().detach().numpy()\n",
    "    # pred_ax = model.decode(z+torch.Tensor(test_arr).to(device)).cpu().detach().numpy() - pred_original\n",
    "    ts_pred = pred_ax.squeeze()[0][row, col, :]\n",
    "    ts_real = data_for_viz.squeeze()[0][row, col, :]\n",
    "    \n",
    "    # Set aside highest and lowest traversal for finding the nodes that change the most (NEXT STAGE)\n",
    "    ax[i].plot(ts_pred, label=\"Predicted\")\n",
    "    ax[i].plot(ts_real, label=\"Original\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7f6898-4dec-4540-9aef-bcf875e866fc",
   "metadata": {},
   "source": [
    "Next, let's further embed the latent space from the VAE such that it only occupies 2 dimensions. This is for plotting purposes. We'll use SparsePCA for now, although other alternatives could include regular PCA, tSNE, or UMAP (preferred for large sample size/many-dimensions scenarios because it is more expressive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39505149-4ec8-440b-acee-1d0deba752c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "group = []\n",
    "for t in clinical['primary_diagnosis'].values:\n",
    "    if t==\"Typically-Developing\":\n",
    "        group.append(0)\n",
    "    elif t==\"ADHD\":\n",
    "        group.append(1)\n",
    "    elif t==\"ASD\":\n",
    "        group.append(2)\n",
    "# clinical.insert(0, \"group\", group)\n",
    "\n",
    "# Add some accounting of where the means are w.r.t. the clinical values - useful later on!!!\n",
    "all_means_df.insert(0, \"num_index\", np.arange(all_means_df.shape[0]))\n",
    "out = pd.merge(left=clinical, right=all_means_df, left_on=\"subject_id\", right_index=True)\n",
    "out.index = out['num_index']\n",
    "out.drop(\"num_index\", axis=1, inplace=True) # Remove column to avoid messing up indices later and, hey it's served its purpose by now\n",
    "\n",
    "limits = 2e-3\n",
    "n_levels = 2\n",
    "embedded = SparsePCA(n_components=2).fit_transform(all_means)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eb096a-e6c0-4709-b8ab-4fe07c6965ce",
   "metadata": {},
   "source": [
    "Plot all embeddings for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635a9e52-1f59-4e64-99b1-4925cd0e3ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.scatter(embedded[:, 0], embedded[:, 1], c=all_groups_num)\n",
    "plot_kde(V=embedded[all_groups_num==1, :], ax=ax, colour='purple', label=None, limits=limits, n_levels=n_levels)\n",
    "plot_kde(V=embedded[all_groups_num==2, :], ax=ax, colour='teal', label=None, limits=limits, n_levels=n_levels)\n",
    "plot_kde(V=embedded[all_groups_num==3, :], ax=ax, colour='yellow', label=None, limits=limits, n_levels=n_levels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c9bd3b-b45b-42ea-aa45-6900b374ac46",
   "metadata": {
    "tags": []
   },
   "source": [
    "Plot only the embeddings for which we have corresponding clinical scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a92c25-bb95-4342-be2b-1a34f9a8f634",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "embedded_short = embedded.copy()[out.index, :]\n",
    "all_means_short = all_means.copy()[out.index, :]\n",
    "all_groups_num_short = all_groups_num[out.index]\n",
    "ax.scatter(embedded_short[:, 0], embedded_short[:, 1], c=all_groups_num_short)\n",
    "plot_kde(V=embedded_short[all_groups_num_short==1, :], ax=ax, colour='purple', label=None, limits=limits, n_levels=n_levels)\n",
    "plot_kde(V=embedded_short[all_groups_num_short==2, :], ax=ax, colour='teal', label=None, limits=limits, n_levels=n_levels)\n",
    "plot_kde(V=embedded_short[all_groups_num_short==3, :], ax=ax, colour='yellow', label=None, limits=limits, n_levels=n_levels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e611d1-ea42-4eed-89b5-7a7ec801310a",
   "metadata": {},
   "source": [
    "Finally, let's take a peek at some canonical correlation analyses (CCA). This technique enables us to inspect the shared latent space between the VAE embeddings and the clincal scores - in essence, to see whether the VAE embeddings capture relevant clinical phenomena!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28d73fa-1b45-411f-b4ae-82ba537dc885",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rs = 20\n",
    "\n",
    "# TODO they advise against using LASSO (i.e., L1) with alpha=0 for \"numerical reasons\" - probably unstable\n",
    "linear_cca = SCCA_IPLS(latent_dimensions = 3, alpha=1, l1_ratio=1, epochs=10000, random_state=rs)\n",
    "train_view_1 = out.iloc[:, 2:20].values\n",
    "train_view_2 = out.iloc[:, 20:].values\n",
    "\n",
    "# linear_cca.fit((train_view_1.astype(float), train_view_2))\n",
    "# linear_cca.pairwise_correlations((train_view_1.astype(float), train_view_2))\n",
    "# weights = linear_cca.weights_\n",
    "\n",
    "# pwcorrs = pg.pairwise_corr(out, columns=[list(out.columns[2:20]), list(out.columns[20:])], method=\"spearman\", padjust=\"fdr_bh\")\n",
    "\n",
    "# Define grid of potential regularization parameters\n",
    "c1 = [0.05, 0.1, 0.3, 0.7, 0.9]\n",
    "c2 = [0.05, 0.1, 0.3, 0.7, 0.9]\n",
    "c3 = [2, 3, 4]\n",
    "param_grid = {'l1_ratio': c1,\n",
    "              'alpha': c2,\n",
    "              'latent_dimensions': c3}\n",
    "\n",
    "cv = 5\n",
    "\n",
    "# Conduct grid search\n",
    "ridge = GridSearchCV(SCCA_IPLS(random_state=rs), param_grid=param_grid,\n",
    "                     cv=cv, verbose=True, scoring=scorer).fit((train_view_1, train_view_2)).best_estimator_\n",
    "\n",
    "projections = ridge.transform((train_view_1, train_view_2))\n",
    "correlation = ridge.score((train_view_1, train_view_2))\n",
    "view_1_weights = ridge.weights_[0]\n",
    "view_2_weights = ridge.weights_[1]\n",
    "\n",
    "# Loadings biplot\n",
    "plt.figure()\n",
    "plt.plot([-1, 1], [0, 0], ls=\":\", c='k')\n",
    "plt.plot([0, 0], [-1, 1], ls=\":\", c='k')\n",
    "view_loadings = ridge.loadings_([train_view_1, train_view_2])\n",
    "plt.scatter(view_loadings[0][:, 0], view_loadings[0][:, 1])\n",
    "plt.scatter(view_loadings[1][:, 0], view_loadings[1][:, 1])\n",
    "for i in range(view_loadings[0].shape[0]):\n",
    "    plt.arrow(0, 0, view_loadings[0][i, 0], view_loadings[0][i, 1], color=\"blue\")\n",
    "    plt.text(view_loadings[0][i, 0], view_loadings[0][i, 1], out.columns[2:20][i], ha=\"center\")\n",
    "for i in range(view_loadings[1].shape[0]):\n",
    "    plt.arrow(0, 0, view_loadings[1][i, 0], view_loadings[1][i, 1], color=\"orange\")\n",
    "\n",
    "# Weights biplots(?)\n",
    "plt.figure()\n",
    "plt.scatter(view_1_weights[:, 0], view_1_weights[:, 1])\n",
    "plt.scatter(view_2_weights[:, 0], view_2_weights[:, 1])\n",
    "\n",
    "plt.figure()\n",
    "all_cols = out.columns\n",
    "plt.scatter(projections[0][:, 0], projections[1][:, 0], c = out.loc[:, out.columns[2]])\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(embedded_short[:, 0], embedded_short[:, 1], c = out.loc[:, out.columns[3]])\n",
    "\n",
    "# Compile results - correlations between latents and SWAN scores\n",
    "res = spearmanr(embedded_short, out.iloc[:, 2:20])\n",
    "res = spearmanr(all_means_short, out.iloc[:, 2:20])\n",
    "res_xcorr_r = pd.DataFrame(data=res[0][latent_dim:, :latent_dim], index=out.columns[2:20], columns=[f\"latent_{i}\" for i in range(latent_dim)])\n",
    "res_xcorr_r.insert(0, \"factor\", [\"Attention\"]*9 + [\"Hyperactivity\"]*9)\n",
    "res_xcorr_p = pd.DataFrame(data=res[1][latent_dim:, :latent_dim], index=out.columns[2:20], columns=[f\"latent_{i}\" for i in range(latent_dim)])\n",
    "res_xcorr_p.insert(0, \"factor\", [0]*9 + [1]*9)\n",
    "\n",
    "os.makedirs(f\"/home/coveneuro-leif/Results/VAE_fMRI_{latent_dim}lt_{epoch}epo/\", exist_ok=True)\n",
    "res_xcorr_r.to_csv(f\"/home/coveneuro-leif/Results/VAE_fMRI_{latent_dim}lt_{epoch}epo/latent_clinical_correlations.csv\")\n",
    "res_xcorr_p.to_csv(f\"/home/coveneuro-leif/Results/VAE_fMRI_{latent_dim}lt_{epoch}epo/latent_clinical_pvals.csv\")\n",
    "\n",
    "# TODO the correlations for latent 2 an 4 recapitulate the hyperactivity items\n",
    "# of the SWAN as defined in the 2-factor model of the SWAN (https://www.frontiersin.org/journals/psychiatry/articles/10.3389/fpsyt.2024.1330716)\n",
    "# Latent 4 is more focused on hyperactivity, latent 2 is involved ith both including attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4e037b-9d40-40a9-bf44-90dc8ec8076f",
   "metadata": {},
   "source": [
    "You can see clearly that some of the latent dimensions (as stored in the res_xcorr_) dataframes correspond strongly to the different factors of the SWAN as defined in peer-reviewed literature. cf. how this compares to the MEG case: relationships with fMRI tend to be somewhat weaker but nevertheless capture the overall factor structure in at least a subset of the VAE latent dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9e31f8-1889-411d-ab21-9a75395782c7",
   "metadata": {},
   "source": [
    "# Okay, time for Part II: MEG\n",
    "\n",
    "The MEG analysis is a bit different, but mainly in the sense that the models have different dims. This is due to the fact that the connectivity matrices have different shapes - both spatially, and temporally. In practice, this just means that the internals of the model need to have slightly different shapes, and the dataloader needs to index the data in a slightly different way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183542f3-48d1-4e3d-a6a0-ec413a7ac712",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv3DAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=2):\n",
    "        super(Conv3DAutoencoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        # inputs_ = inputs.cpu()#.detach().numpy()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1),  # Output: (16, 75, 75, 10)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=2),  # Output: (16, 37, 37, 5)\n",
    "\n",
    "            nn.Conv3d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),  # Output: (32, 37, 37, 5)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=2)  # Output: (32, 18, 18, 2)\n",
    "        )\n",
    "        \n",
    "        ldim = 32*18*18*10\n",
    "        self.fc_mu = nn.Linear(ldim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(ldim, latent_dim)\n",
    "        self.fc_decode = nn.Linear(latent_dim, ldim)\n",
    "        # x = fc_mu(x.view(x.size(0), -1))\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose3d(in_channels=32, out_channels=16, kernel_size=2, stride=2),  # Output: (16, 36, 36, 4)\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.ConvTranspose3d(in_channels=16, out_channels=8, kernel_size=2, stride=2),  # Output: (8, 72, 72, 8)\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.ConvTranspose3d(in_channels=8, out_channels=1, kernel_size=(6,6,3), stride=(1,1,1), padding=(1,1,1)),  # Output: (1, 75, 75, 10)\n",
    "            nn.Sigmoid()  # Output values between 0 and 1\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)  # Standard deviation\n",
    "        eps = torch.randn_like(std)  # Random noise\n",
    "        return mu + eps * std  # Reparameterization trick\n",
    "\n",
    "    def decode(self, z):\n",
    "        x = self.fc_decode(z)\n",
    "        x = x.view(-1, 32, 18, 18, 10)  # Reshape to (32, 18, 18, 2)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z)\n",
    "        return recon, mu, logvar\n",
    "\n",
    "class Custom3DDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (np.ndarray or list): List or numpy array of shape (num_samples, 75, 75, 10).\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        self.file_list = [f for f in os.listdir(data_dir) if f.endswith('.npy')] \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # idx=0\n",
    "        # file_path = os.path.join(data_dir, file_list[idx])\n",
    "        file_path = os.path.join(self.data_dir, self.file_list[idx])\n",
    "        # file_path = \"/home/coveneuro-leif/Downloads/dl_project_broad/PND03_HSC_0689_SE01MEG_task-Rest_run-1_meg_broad.npy\"\n",
    "        # sample = np.load(file_path)[:, :, :8]\n",
    "        sample = np.load(file_path)[:, :, :40]\n",
    "\n",
    "        # Convert to a PyTorch tensor and add a channel dimension\n",
    "        sample = torch.tensor(sample, dtype=torch.float32).unsqueeze(0)  # Shape: (1, 75, 75, 10)\n",
    "        \n",
    "        return sample, file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b855c7e8-d08f-44dc-9af3-83021ed2e0e7",
   "metadata": {},
   "source": [
    "Let's train the model. Note that in this case, there is an additional outer loop to iterate over frequency bands. This is helpful for allowing us to inspect band-level differentiability of the groups. It is otherwise the same logic as the fMRI case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47c8b8e-b7dc-4a70-8104-759ca11b54d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for band in [\"theta\", \"alpha\", \"beta\"]:\n",
    "    # band=\"alpha\"\n",
    "    data_dir = f\"/home/coveneuro-leif/Downloads/dl_project_{band}/\"\n",
    "    dataset = Custom3DDataset(data_dir)\n",
    "    \n",
    "    batch_size = 16\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    for latent_dim in [4, 8, 16, 32, 64]:\n",
    "        num_epochs = 1001 # 1001 so that the 1000th epoch is also saved. THANKS PYTHON\n",
    "        \n",
    "        # Instantiate the model\n",
    "        model = Conv3DAutoencoder(latent_dim=latent_dim)\n",
    "        model.to(device)\n",
    "        \n",
    "        # Training setup\n",
    "        criterion = nn.MSELoss()  # Mean squared error loss for reconstruction\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-3)  # Adam optimizer\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        for epoch in np.arange(1, num_epochs): # Again, to make sure we can train for EXACTLY 1000 epochs (i.e., not save the 101st...201st...etc. Not that it would matter too much)\n",
    "            running_loss = 0.0\n",
    "        \n",
    "            for inputs, _ in dataloader:\n",
    "                # break\n",
    "                inputs = inputs+inputs.permute(0,1,3,2,4)\n",
    "                # Move data to GPU if available\n",
    "                inputs = inputs.to(device)\n",
    "                # break\n",
    "                # Zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "                # Forward pass\n",
    "                outputs = model(inputs) # outputs[0].shape\n",
    "                \n",
    "                # Compute the loss (between the input and its reconstruction)\n",
    "                loss = criterion(outputs[0], inputs) + criterion(outputs[0].permute(0,1,3,2,4), inputs)\n",
    "                \n",
    "                # Backpropagation and optimization\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "                # Accumulate loss\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            if epoch%100==0:\n",
    "                torch.save(model.state_dict(), f\"/home/coveneuro-leif/Results/3DCNN_AE_MEG_{latent_dim}_{epoch}_{band}.pt\")\n",
    "        \n",
    "            # Print average loss for the epoch\n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            print(f'Epoch {epoch}/{num_epochs}, Loss: {epoch_loss:.6f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0679b49a-0a0f-4582-900f-ac2d17fdebf1",
   "metadata": {},
   "source": [
    "Grab anatomical labels like we did before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e2c033-4894-41b7-9136-56b0994b23dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = mne.datasets.sample.data_path()\n",
    "subject = \"sample\"\n",
    "data_dir = data_path / \"MEG\" / subject\n",
    "subjects_dir = data_path / \"subjects\"\n",
    "labels = mne.read_labels_from_annot(\"sample\", parc=\"aparc\", subjects_dir=subjects_dir)\n",
    "labels_vol = [\n",
    "    \"Left-Amygdala\",\n",
    "    \"Left-Thalamus-Proper\",\n",
    "    \"Left-Cerebellum-Cortex\",\n",
    "    \"Brain-Stem\",\n",
    "    \"Right-Amygdala\",\n",
    "    \"Right-Thalamus-Proper\",\n",
    "    \"Right-Cerebellum-Cortex\",\n",
    "]\n",
    "\n",
    "label_names = [label.name for label in labels] + labels_vol\n",
    "lh_labels = [name for name in label_names if name.endswith(\"lh\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825005b2-e4e4-4326-9620-15eb299e16d3",
   "metadata": {},
   "source": [
    "Latent space plotting as before for fMRI. You will note that generally the separation is better, and is the stringest for alpha- and beta bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fddb8a4-02bc-4c56-bb3f-fad2c6e06984",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "band = \"alpha\"\n",
    "data_dir = f\"/home/coveneuro-leif/Downloads/dl_project_{band}/\"\n",
    "dataset = Custom3DDataset(data_dir)\n",
    "batch_size = 16\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Plot a cascade of latent traversal\n",
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)  # Standard deviation\n",
    "    eps = torch.randn_like(std)  # Random noise\n",
    "    return mu + eps * std  # Reparameterization trick\n",
    "\n",
    "# We saved these earlier during training...dealer's choice as to which can be used\n",
    "# Epochs must be in increments of 100 only \n",
    "latent_dim = 16\n",
    "epoch = 1000\n",
    "model = Conv3DAutoencoder(latent_dim=latent_dim)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(f\"/home/coveneuro-leif/Results/3DCNN_AE_MEG_{latent_dim}_{epoch}.pt\"))\n",
    "\n",
    "model.eval()\n",
    "means = []\n",
    "vars_ = []\n",
    "names_data = []\n",
    "groups_data = []\n",
    "for data, name in dataloader:\n",
    "    data = torch.Tensor(data).to(device)  # Send data to GPU if available\n",
    "    out = model.encode(data)\n",
    "    means.append(out[0].cpu().detach().numpy())\n",
    "    vars_.append(out[1].cpu().detach().numpy())\n",
    "    names_data.extend(name)\n",
    "    \n",
    "    for n in name:\n",
    "        if \"Typically-Developing\" in n:\n",
    "            groups_data.append(\"Typically-Developing\")\n",
    "        elif \"ASD\" in n:\n",
    "            groups_data.append(\"ASD\")\n",
    "        elif \"ADHD\" in n:\n",
    "            groups_data.append(\"ADHD\")\n",
    "        else:\n",
    "            groups_data.append(\"NA\")\n",
    "\n",
    "clinical = pd.read_csv(\"/home/coveneuro-leif/clinical_SWAN.csv\")\n",
    "names_data = np.array(names_data)\n",
    "# all_files = glob.glob(\"/home/coveneuro-leif/Downloads/ae_data/*.npy\")\n",
    "all_names = [\"_\".join(a.split(\"/\")[-1].split(\".\")[0].split(\"_\")[:3]) for a in names_data]\n",
    "all_means = np.concatenate(means)\n",
    "all_groups = np.array(groups_data)\n",
    "\n",
    "# Numerical group names so that plotting can be done more easily\n",
    "all_groups_num = []\n",
    "for a in all_groups:\n",
    "    if \"Typically-Developing\" in a:\n",
    "        all_groups_num.append(1)\n",
    "    if \"ADHD\" in a:\n",
    "        all_groups_num.append(2)\n",
    "    if \"ASD\" in a:\n",
    "        all_groups_num.append(3)\n",
    "    if \"NA\" in a:\n",
    "        all_groups_num.append(np.nan)\n",
    "        \n",
    "all_groups_num = np.array(all_groups_num)        \n",
    "all_means_df = pd.DataFrame(data=all_means.astype(float), index=all_names, columns=[f\"lat{i}\" for i in range(all_means.shape[1])])\n",
    "\n",
    "# TODO use this to sort latents for plotting timeseries\n",
    "from scipy.stats import ttest_ind\n",
    "stats_results = []\n",
    "for i in range(latent_dim):\n",
    "    var = f\"lat{i}\"\n",
    "    # plt.figure()\n",
    "    # plt.boxplot(positions=[0], x=all_means_df[var][all_groups_num==1])\n",
    "    # plt.boxplot(positions=[1], x=all_means_df[var][all_groups_num==2])\n",
    "    # plt.boxplot(positions=[2], x=all_means_df[var][all_groups_num==3])\n",
    "    stat_1_2, pval_1_2 = ttest_ind(all_means_df[var][all_groups_num==1], all_means_df[var][all_groups_num==2])\n",
    "    stat_2_3, pval_2_3 = ttest_ind(all_means_df[var][all_groups_num==2], all_means_df[var][all_groups_num==3])\n",
    "    stat_1_3, pval_1_3 = ttest_ind(all_means_df[var][all_groups_num==1], all_means_df[var][all_groups_num==3])\n",
    "    stats_results.append((pval_1_2, pval_2_3, pval_1_3, stat_1_2, stat_2_3, stat_1_3))\n",
    "stats_results_df = pd.DataFrame(data=stats_results, \n",
    "                                columns=[\"p-TD_ADHD\", \"p-ADHD_ASD\", \"p-TD_ASD\", \"t-TD_ADHD\", \"t-ADHD_ASD\", \"t-TD_ASD\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e47c92-81b0-4c5c-94e0-aa96e71ee3e6",
   "metadata": {},
   "source": [
    "Plots of reconstructed connectivity matrices. Right out of the gate, it is clear that the matrices, compared to fMRI, are more symmetrical and more well-behaved in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545cd777-8322-4056-9dbe-24212d3dacda",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_r, max_c = np.unravel_index(np.argmax(np.abs(main_nodes)), main_nodes.shape)\n",
    "\n",
    "# Plot reconstructed connectivity matrices\n",
    "predicted = model(data)[0]\n",
    "predicted = predicted.cpu().detach().numpy()\n",
    "data_for_viz = data\n",
    "data_for_viz = data_for_viz+data_for_viz.permute(0,1,3,2,4)\n",
    "data_for_viz = data_for_viz.cpu().detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "plt.subplots_adjust(left=0.10, right=0.99, top=0.95, bottom=0.20, wspace=0.5)\n",
    "ax[0].set_title(\"Predicted\")\n",
    "ax[1].set_title(\"Original\")\n",
    "ax[0].imshow(predicted.squeeze()[0][:, :, 0], vmin=0, vmax=1)\n",
    "ax[1].imshow(data_for_viz.squeeze()[0][:, :, 0], vmin=0, vmax=1)\n",
    "ax[0].set_xticks(np.arange(len(label_names)), label_names, rotation=90, fontsize=8)\n",
    "ax[0].set_yticks(np.arange(len(label_names)), label_names, rotation=00, fontsize=8)\n",
    "ax[1].set_xticks(np.arange(len(label_names)), label_names, rotation=90, fontsize=8)\n",
    "ax[1].set_yticks(np.arange(len(label_names)), label_names, rotation=00, fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da71546-6aa8-4e51-9548-6a4e39462b4f",
   "metadata": {},
   "source": [
    "Plots of reconstructed volumes. Again, it's pretty cool! You'll note right away that the reconstruction performs better here than it did with fMRI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6688d47-f9d3-4392-ba77-1dd570146c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot timeseries of reconstructed 3D volumes\n",
    "fig, ax = plt.subplots(1, num_traversal) # Top row: ax0 traversal, Bottom row: ax1 traversal\n",
    "row = max_r\n",
    "col = max_c\n",
    "row_name = label_names[row]\n",
    "col_name = label_names[col]\n",
    "ts_combined_title = f\"Connectivity: {row_name}  {col_name}\"\n",
    "plt.suptitle(ts_combined_title)\n",
    "\n",
    "for i in range(num_traversal):\n",
    "    test_arr = np.zeros(all_means.shape[1])\n",
    "    test_arr[latent_perturb] = add_vals[i]\n",
    "    \n",
    "    # Extract most significant latent timeseries\n",
    "    pred_ax = model.decode(z+torch.Tensor(test_arr).to(device)).cpu().detach().numpy()\n",
    "    # pred_ax = model.decode(z+torch.Tensor(test_arr).to(device)).cpu().detach().numpy() - pred_original\n",
    "    ts_pred = pred_ax.squeeze()[0][row, col, :]\n",
    "    ts_real = data_for_viz.squeeze()[0][row, col, :]\n",
    "    \n",
    "    # Set aside highest and lowest traversal for finding the nodes that change the most (NEXT STAGE)\n",
    "    ax[i].plot(ts_pred, label=\"Predicted\")\n",
    "    ax[i].plot(ts_real, label=\"Original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6034763-b24f-46ed-960e-f11c4a98409c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Plots of embeddings using SparsePCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb766725-34bf-4f63-b861-1c67adf7779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "group = []\n",
    "for t in clinical['primary_diagnosis'].values:\n",
    "    if t==\"Typically-Developing\":\n",
    "        group.append(0)\n",
    "    elif t==\"ADHD\":\n",
    "        group.append(1)\n",
    "    elif t==\"ASD\":\n",
    "        group.append(2)\n",
    "# clinical.insert(0, \"group\", group)\n",
    "\n",
    "# Add some accounting of where the means are w.r.t. the clinical values - useful later on!!!\n",
    "all_means_df.insert(0, \"num_index\", np.arange(all_means_df.shape[0]))\n",
    "out = pd.merge(left=clinical, right=all_means_df, left_on=\"subject_id\", right_index=True)\n",
    "out.index = out['num_index']\n",
    "out.drop(\"num_index\", axis=1, inplace=True) # Remove column to avoid messing up indices later and, hey it's served its purpose by now\n",
    "\n",
    "limits = 2e-3\n",
    "n_levels = 2\n",
    "embedded = SparsePCA(n_components=2).fit_transform(all_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f48227-3bf7-4888-a983-25333e5f346b",
   "metadata": {},
   "source": [
    "Plotting all embeddings by group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6b8925-51d9-4e3b-a926-332752fe71fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "ax.scatter(embedded[:, 0], embedded[:, 1], c=all_groups_num)\n",
    "plot_kde(V=embedded[all_groups_num==1, :], ax=ax, colour='purple', label=None, limits=limits, n_levels=n_levels)\n",
    "plot_kde(V=embedded[all_groups_num==2, :], ax=ax, colour='teal', label=None, limits=limits, n_levels=n_levels)\n",
    "plot_kde(V=embedded[all_groups_num==3, :], ax=ax, colour='yellow', label=None, limits=limits, n_levels=n_levels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dc62c1-0bd0-4b39-8f3a-fd7ca0443128",
   "metadata": {},
   "source": [
    "Plotting where we have clinical scores only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefe1ee0-c743-447c-ba7a-40bab4e05fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "fig, ax = plt.subplots(1,1)\n",
    "embedded_short = embedded.copy()[out.index, :]\n",
    "all_means_short = all_means.copy()[out.index, :]\n",
    "all_groups_num_short = all_groups_num[out.index]\n",
    "ax.scatter(embedded_short[:, 0], embedded_short[:, 1], c=all_groups_num_short)\n",
    "plot_kde(V=embedded_short[all_groups_num_short==1, :], ax=ax, colour='purple', label=None, limits=limits, n_levels=n_levels)\n",
    "plot_kde(V=embedded_short[all_groups_num_short==2, :], ax=ax, colour='teal', label=None, limits=limits, n_levels=n_levels)\n",
    "plot_kde(V=embedded_short[all_groups_num_short==3, :], ax=ax, colour='yellow', label=None, limits=limits, n_levels=n_levels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6f88f9-c738-4310-91d9-f38de072d0c3",
   "metadata": {},
   "source": [
    "Plotting CCA as we did with fMRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce2ba9a-6a8f-4dce-a646-cac5fd83a537",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = 20\n",
    "\n",
    "# TODO they advise against using LASSO (i.e., L1) with alpha=0 for \"numerical reasons\" - probably unstable\n",
    "linear_cca = SCCA_IPLS(latent_dimensions = 3, alpha=1, l1_ratio=1, epochs=10000, random_state=rs)\n",
    "train_view_1 = out.iloc[:, 2:20].values\n",
    "train_view_2 = out.iloc[:, 20:].values\n",
    "\n",
    "linear_cca.fit((train_view_1.astype(float), train_view_2))\n",
    "linear_cca.pairwise_correlations((train_view_1.astype(float), train_view_2))\n",
    "weights = linear_cca.weights_\n",
    "\n",
    "# pwcorrs = pg.pairwise_corr(out, columns=[list(out.columns[2:20]), list(out.columns[20:])], method=\"spearman\", padjust=\"fdr_bh\")\n",
    "\n",
    "# Define grid of potential regularization parameters\n",
    "c1 = [0.05, 0.1, 0.3, 0.7, 0.9]\n",
    "c2 = [0.05, 0.1, 0.3, 0.7, 0.9]\n",
    "c3 = [2, 3, 4]\n",
    "param_grid = {'l1_ratio': c1,\n",
    "              'alpha': c2,\n",
    "              'latent_dimensions': c3}\n",
    "\n",
    "cv = 5\n",
    "\n",
    "# Conduct grid search\n",
    "ridge = GridSearchCV(SCCA_IPLS(random_state=rs), param_grid=param_grid,\n",
    "                     cv=cv, verbose=True, scoring=scorer).fit((train_view_1, train_view_2)).best_estimator_\n",
    "\n",
    "projections = ridge.transform((train_view_1, train_view_2))\n",
    "correlation = ridge.score((train_view_1, train_view_2))\n",
    "view_1_weights = ridge.weights_[0]\n",
    "view_2_weights = ridge.weights_[1]\n",
    "\n",
    "# Loadings biplot\n",
    "plt.figure()\n",
    "view_loadings = ridge.loadings_([train_view_1, train_view_2])\n",
    "plt.scatter(view_loadings[0][:, 0], view_loadings[0][:, 1])\n",
    "plt.scatter(view_loadings[1][:, 0], view_loadings[1][:, 1])\n",
    "\n",
    "# Weights biplots(?)\n",
    "plt.figure()\n",
    "plt.scatter(view_1_weights[:, 0], view_1_weights[:, 1])\n",
    "plt.scatter(view_2_weights[:, 0], view_2_weights[:, 1])\n",
    "\n",
    "plt.figure()\n",
    "all_cols = out.columns\n",
    "plt.scatter(projections[0][:, 0], projections[1][:, 0], c = out.loc[:, out.columns[2]])\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(embedded_short[:, 0], embedded_short[:, 1], c = out.loc[:, out.columns[3]])\n",
    "\n",
    "# Compile results - correlations between latents and SWAN scores\n",
    "res = spearmanr(embedded_short, out.iloc[:, 2:20])\n",
    "res = spearmanr(all_means_short, out.iloc[:, 2:20])\n",
    "res_xcorr_r = pd.DataFrame(data=res[0][latent_dim:, :latent_dim], index=out.columns[2:20], columns=[f\"latent_{i}\" for i in range(latent_dim)])\n",
    "res_xcorr_r.insert(0, \"factor\", [0]*9 + [1]*9)\n",
    "res_xcorr_p = pd.DataFrame(data=res[1][latent_dim:, :latent_dim], index=out.columns[2:20], columns=[f\"latent_{i}\" for i in range(latent_dim)])\n",
    "res_xcorr_p.insert(0, \"factor\", [0]*9 + [1]*9)\n",
    "\n",
    "os.makedirs(f\"/home/coveneuro-leif/Results/VAE_{latent_dim}lt_{epoch}epo/\", exist_ok=True)\n",
    "res_xcorr_r.to_csv(f\"/home/coveneuro-leif/Results/VAE_{latent_dim}lt_{epoch}epo/latent_clinical_correlations.csv\")\n",
    "res_xcorr_p.to_csv(f\"/home/coveneuro-leif/Results/VAE_{latent_dim}lt_{epoch}epo/latent_clinical_pvals.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fede30-47ef-46ab-9d91-34f63a1c16fe",
   "metadata": {},
   "source": [
    "# TADAHH! That's all\n",
    "We have walked through a comprehensive analysis of brain activity data from start through preprocessing to eventual downstream analysis. There are many more ways to explore these rich datasets, and we intentionally glossed over some of the basic parts that were done at the very start - exploratory analysis, subjective initial plotting of data and identifying trends - in service of providing a comprehensive overview.\n",
    "\n",
    "## What did we learn?\n",
    "We learned a lot of interesting things not only about ML but also ADHD and ASD brain activity patterns, and how they relate to typically-developing children and clinical ratings of function throughout development. We generally infer from this set that individuals in the ADHD and ASD groups both tend to have distinct subgroups within them that differ from each other and from typically-developing children. Much more work will be required to explore these findings in depth, but it demonstrates the great promise that neuroimaging and neurophysiological data analysis have in characterizing developmental disorders, and may help us point the way towards better therapies for these individuals. Thanks for following along!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:leif-env]",
   "language": "python",
   "name": "conda-env-leif-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
